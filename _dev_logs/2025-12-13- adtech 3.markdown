---
layout: post
title: "Day 3: ClickHouse ì‹¤ì‹œê°„ ë°ì´í„° ì ì¬ ë° ë¶„ì„ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
date: 2025-12-13
typora-root-url: ../
image_style: "max-width:80%; display:block; margin:1em auto; border-radius:10px; box-shadow:2px 2px 8px rgba(0,0,0,0.8);"
---



# Day 3: ClickHouse ì‹¤ì‹œê°„ ë°ì´í„° ì ì¬ ë° ë¶„ì„ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

## ğŸ“‹ ì˜¤ëŠ˜ì˜ ëª©í‘œ

Day 2ì—ì„œ êµ¬ì¶•í•œ Spark Streaming ì§‘ê³„ ê²°ê³¼ë¥¼ ClickHouseì— ì €ì¥í•˜ì—¬ **ì™„ì „í•œ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸**ì„ ì™„ì„±í•œë‹¤.

------

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

```
[ê´‘ê³  ë¡œê·¸ ìƒì„±ê¸°] 
    â†“ (JSON events)
[Kafka: ad-events Topic]
    â†“ (Stream)
[Spark Streaming: 1ë¶„ ìœˆë„ìš° ì§‘ê³„]
    â†“ (Aggregated data)
[ClickHouse: OLAP ì €ì¥ì†Œ] â† âœ¨ Today's Focus
    â†“ (Query)
[ë¶„ì„ & ëŒ€ì‹œë³´ë“œ]
```

------

## ğŸš€ Step 1: ClickHouse í…Œì´ë¸” ì„¤ê³„

### 1.1 ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ë¶„ì„

AdTech ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ì—ì„œ ì €ì¥í•´ì•¼ í•  í•µì‹¬ ë©”íŠ¸ë¦­:

- **ì‹œê°„ ìœˆë„ìš°**: 1ë¶„ ë‹¨ìœ„ ì§‘ê³„ ì‹œì‘/ì¢…ë£Œ ì‹œê°„
- **ì°¨ì›(Dimensions)**: event_type, country, ad_format
- **ë©”íŠ¸ë¦­(Metrics)**: event_count, avg_revenue, total_revenue

### 1.2 í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì„¤ê³„

```sql
CREATE DATABASE IF NOT EXISTS rtb;

CREATE TABLE IF NOT EXISTS rtb.ad_events_aggregated (
    window_start DateTime,
    window_end DateTime,
    event_type String,
    country String,
    ad_format String,
    event_count UInt64,
    avg_revenue Float64,
    total_revenue Float64,
    created_at DateTime DEFAULT now()
) ENGINE = MergeTree()
ORDER BY (window_start, event_type, country, ad_format)
PARTITION BY toYYYYMM(window_start);
```

**ì„¤ê³„ í¬ì¸íŠ¸:**

1. **MergeTree ì—”ì§„**: ClickHouseì˜ ê¸°ë³¸ ê³ ì„±ëŠ¥ OLAP ì—”ì§„
   - ì‚½ì… ì‹œ ìë™ ì •ë ¬ ë° ë³‘í•©
   - ë²”ìœ„ ì¿¼ë¦¬ ìµœì í™”
2. **ORDER BY ì ˆ**: ì¿¼ë¦¬ íŒ¨í„´ì— ìµœì í™”ëœ ì •ë ¬ í‚¤
   - `window_start`: ì‹œê³„ì—´ ì¿¼ë¦¬ (ìµœê·¼ ë°ì´í„° ì¡°íšŒ)
   - `event_type, country, ad_format`: ì°¨ì›ë³„ í•„í„°ë§
3. **PARTITION BY**: ì›”ë³„ íŒŒí‹°ì…”ë‹
   - ë°ì´í„° ê´€ë¦¬ ìš©ì´ (ì˜¤ë˜ëœ íŒŒí‹°ì…˜ ì‚­ì œ)
   - ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ (íŒŒí‹°ì…˜ í”„ë£¨ë‹)

### 1.3 ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

`sql/create_tables.sql` íŒŒì¼ ìƒì„±:

```sql
-- ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ë¨¼ì €!)
CREATE DATABASE IF NOT EXISTS rtb;

-- ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì§‘ê³„ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS rtb.ad_events_aggregated (
    window_start DateTime,
    window_end DateTime,
    event_type String,
    country String,
    ad_format String,
    event_count UInt64,
    avg_revenue Float64,
    total_revenue Float64,
    created_at DateTime DEFAULT now()
) ENGINE = MergeTree()
ORDER BY (window_start, event_type, country, ad_format)
PARTITION BY toYYYYMM(window_start);

-- Raw ì´ë²¤íŠ¸ í…Œì´ë¸” (í–¥í›„ ì‚¬ìš©)
CREATE TABLE IF NOT EXISTS rtb.ad_events_raw (
    timestamp DateTime,
    event_type String,
    user_id String,
    country String,
    ad_format String,
    campaign_id String,
    bid_price Float64,
    revenue Float64,
    created_at DateTime DEFAULT now()
) ENGINE = MergeTree()
ORDER BY (timestamp, event_type);
```

------

## ğŸ”§ Step 2: Docker í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 2.1 Kafka ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°

**ë¬¸ì œ ë°œìƒ:**

```
NodeExistsException: KeeperErrorCode = NodeExists
```

Zookeeperì— ì´ì „ Kafka ë¸Œë¡œì»¤ ì •ë³´ê°€ ë‚¨ì•„ìˆì–´ ì¶©ëŒ ë°œìƒ.

**í•´ê²° ë°©ë²•:**

```bash
# ì™„ì „ ì´ˆê¸°í™”
docker-compose down
sudo rm -rf clickhouse-data/* redis-data/*
docker-compose up -d
sleep 30  # Kafka ì™„ì „ ì‹œì‘ ëŒ€ê¸°
```

### 2.2 ClickHouse ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í™•ì¸

**ë¬¸ì œ:** ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ê°€ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì—†ì´ í…Œì´ë¸”ì„ ë§Œë“¤ë ¤ê³  ì‹œë„.

**í•´ê²°:** SQL íŒŒì¼ì— `CREATE DATABASE IF NOT EXISTS rtb;` ì¶”ê°€ í›„ ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘.

```bash
docker-compose restart clickhouse
docker logs adtech-clickhouse --tail 30
```

### 2.3 í…Œì´ë¸” ìƒì„± í™•ì¸

```sql
-- ClickHouse í´ë¼ì´ì–¸íŠ¸ ì ‘ì†
docker exec -it adtech-clickhouse clickhouse-client

-- í™•ì¸
SHOW DATABASES;
USE rtb;
SHOW TABLES;
DESCRIBE ad_events_aggregated;
```

------

## ğŸ’» Step 3: Spark ClickHouse ì ì¬ íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### 3.1 ìƒˆë¡œìš´ Spark ì• í”Œë¦¬ì¼€ì´ì…˜ ì‘ì„±

`src/spark_processor_clickhouse.py`:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Spark Session ìƒì„± (ClickHouse JDBC ë“œë¼ì´ë²„ í¬í•¨)
spark = SparkSession.builder \
    .appName("RTB-Pipeline-Processor-ClickHouse") \
    .config("spark.jars.packages", 
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
            "com.clickhouse:clickhouse-jdbc:0.4.6:all") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark Session ì‹œì‘ë¨!")

# ê´‘ê³  ë¡œê·¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
ad_log_schema = StructType([
    StructField("event_id", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("user_id", StringType(), True),
    StructField("campaign_id", StringType(), True),
    StructField("ad_format", StringType(), True),
    StructField("country", StringType(), True),
    StructField("device", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("revenue", DoubleType(), True),
])

print("ğŸ“¡ Kafkaì—ì„œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")

# Kafkaì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì½ê¸°
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:29092") \
    .option("subscribe", "ad-events") \
    .option("startingOffsets", "earliest") \
    .load()

# JSON íŒŒì‹±
parsed_df = df.select(
    from_json(col("value").cast("string"), ad_log_schema).alias("data")
).select("data.*")

# íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
processed_df = parsed_df.withColumn(
    "event_timestamp",
    to_timestamp(col("timestamp"))
)

print("ğŸ”„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ!")

# ìœˆë„ìš° ê¸°ë°˜ ì§‘ê³„ (1ë¶„ ë‹¨ìœ„)
windowed_stats = processed_df \
    .withWatermark("event_timestamp", "1 minute") \
    .groupBy(
        window(col("event_timestamp"), "1 minute"),
        col("event_type"),
        col("country"),
        col("ad_format")
    ).agg(
        count("*").alias("event_count"),
        avg("revenue").alias("avg_revenue"),
        sum("revenue").alias("total_revenue")
    )

# ì¶œë ¥ í¬ë§· ë³€í™˜
output_df = windowed_stats.select(
    col("window.start").alias("window_start"),
    col("window.end").alias("window_end"),
    col("event_type"),
    col("country"),
    col("ad_format"),
    col("event_count"),
    round(col("avg_revenue"), 4).alias("avg_revenue"),
    round(col("total_revenue"), 4).alias("total_revenue")
)

# ClickHouse ì—°ê²° ì •ë³´
clickhouse_url = "jdbc:clickhouse://adtech-clickhouse:8123/rtb"
clickhouse_properties = {
    "driver": "com.clickhouse.jdbc.ClickHouseDriver",
    "user": "default",
    "password": ""
}

# ClickHouseì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def write_to_clickhouse(batch_df, batch_id):
    print(f"ğŸ“ Batch {batch_id} ì²˜ë¦¬ ì¤‘... (ë ˆì½”ë“œ ìˆ˜: {batch_df.count()})")
    
    if batch_df.count() > 0:
        batch_df.write \
            .format("jdbc") \
            .option("url", clickhouse_url) \
            .option("dbtable", "ad_events_aggregated") \
            .option("driver", clickhouse_properties["driver"]) \
            .option("user", clickhouse_properties["user"]) \
            .option("password", clickhouse_properties["password"]) \
            .mode("append") \
            .save()
        
        print(f"âœ… Batch {batch_id} ClickHouse ì ì¬ ì™„ë£Œ!")
        
        # ì ì¬ëœ ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥
        batch_df.show(5, truncate=False)

print("ğŸ“Š ì‹¤ì‹œê°„ ì§‘ê³„ ì‹œì‘ - ClickHouse ì ì¬ ëª¨ë“œ")

# ClickHouseë¡œ ìŠ¤íŠ¸ë¦¬ë° (foreachBatch ì‚¬ìš©)
query = output_df \
    .writeStream \
    .outputMode("update") \
    .foreachBatch(write_to_clickhouse) \
    .trigger(processingTime="10 seconds") \
    .start()

print("ğŸš€ Spark Streaming ì‹¤í–‰ ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
query.awaitTermination()
```

### 3.2 í•µì‹¬ êµ¬í˜„ í¬ì¸íŠ¸

**1. foreachBatch íŒ¨í„´ ì‚¬ìš©**

```python
def write_to_clickhouse(batch_df, batch_id):
    # ê° ë§ˆì´í¬ë¡œ ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œë¨
    batch_df.write.format("jdbc").mode("append").save()
```

**ì¥ì :**

- ë°°ì¹˜ ë‹¨ìœ„ íŠ¸ëœì­ì…˜ ì œì–´
- ì—ëŸ¬ í•¸ë“¤ë§ ìš©ì´
- ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥

**2. JDBC ì—°ê²° ì„¤ì •**

```python
clickhouse_url = "jdbc:clickhouse://adtech-clickhouse:8123/rtb"
```

- Docker ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ ì£¼ì†Œ ì‚¬ìš© (`adtech-clickhouse`)
- HTTP í¬íŠ¸ 8123 ì‚¬ìš© (ClickHouse JDBC ê¸°ë³¸)

**3. outputMode = "update"**

- ìœˆë„ìš° ì§‘ê³„ ê²°ê³¼ì˜ ì—…ë°ì´íŠ¸ë§Œ ì „ì†¡
- Complete ëª¨ë“œë³´ë‹¤ íš¨ìœ¨ì 

------

## ğŸ¯ Step 4: Spark Submit ì‹¤í–‰ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 4.1 Maven Dependency ë¬¸ì œ í•´ê²°

**ì²« ë²ˆì§¸ ì‹œë„ (ì‹¤íŒ¨):**

```bash
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.4.6:all
```

**ì—ëŸ¬:**

```
requirement failed: Provided Maven Coordinates must be in the form 'groupId:artifactId:version'
```

`:all` classifierë¥¼ Sparkê°€ ì¸ì‹í•˜ì§€ ëª»í•¨.

**í•´ê²° ë°©ë²•:**

```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  --jars https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc/0.4.6/clickhouse-jdbc-0.4.6-all.jar \
  /opt/spark-apps/spark_processor_clickhouse.py
```

`--jars` ì˜µì…˜ìœ¼ë¡œ ì§ì ‘ JAR ë‹¤ìš´ë¡œë“œ.

### 4.2 ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤

**í„°ë¯¸ë„ 1: ê´‘ê³  ë¡œê·¸ ìƒì„±ê¸°**

```bash
python data-generator/ad_log_generator.py
```

**í„°ë¯¸ë„ 2: Spark Streaming**

```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  --jars https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc/0.4.6/clickhouse-jdbc-0.4.6-all.jar \
  /opt/spark-apps/spark_processor_clickhouse.py
```

**ì •ìƒ ì‹¤í–‰ ë¡œê·¸:**

```
âœ… Spark Session ì‹œì‘ë¨!
ğŸ“¡ Kafkaì—ì„œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...
ğŸ”„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ!
ğŸ“Š ì‹¤ì‹œê°„ ì§‘ê³„ ì‹œì‘ - ClickHouse ì ì¬ ëª¨ë“œ
ğŸš€ Spark Streaming ì‹¤í–‰ ì¤‘...

ğŸ“ Batch 1 ì²˜ë¦¬ ì¤‘... (ë ˆì½”ë“œ ìˆ˜: 42)
âœ… Batch 1 ClickHouse ì ì¬ ì™„ë£Œ!
+-------------------+-------------------+----------+-------+-------------+
|window_start       |window_end         |event_type|country|ad_format    |
+-------------------+-------------------+----------+-------+-------------+
|2025-12-16 23:43:00|2025-12-16 23:44:00|impression|FR     |interstitial |
|2025-12-16 23:43:00|2025-12-16 23:44:00|click     |CA     |interstitial |
+-------------------+-------------------+----------+-------+-------------+
```

------

## ğŸ“Š Step 5: ë°ì´í„° ê²€ì¦ ë° ë¶„ì„

### 5.1 ì‹¤ì‹œê°„ ë°ì´í„° ì ì¬ í™•ì¸

**í„°ë¯¸ë„ 3: ClickHouse ì¿¼ë¦¬**

```bash
docker exec -it adtech-clickhouse clickhouse-client
USE rtb;

-- ì´ ë ˆì½”ë“œ ìˆ˜
SELECT count(*) FROM ad_events_aggregated;
-- ê²°ê³¼: 449 rows (ì‹¤ì‹œê°„ ì¦ê°€ ì¤‘)

-- ìµœê·¼ ë°ì´í„° í™•ì¸
SELECT * FROM ad_events_aggregated 
ORDER BY window_start DESC 
LIMIT 10;
```

**ê²°ê³¼:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€window_startâ”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€window_endâ”€â”¬â”€event_typeâ”€â”¬â”€countryâ”€â”¬â”€ad_formatâ”€â”€â”€â”€â”€â”¬â”€event_countâ”€â”¬â”€avg_revenueâ”€â”¬â”€total_revenueâ”€â”
â”‚ 2025-12-16 23:44:00 â”‚ 2025-12-16 23:45:00 â”‚ click      â”‚ US      â”‚ rewarded_videoâ”‚          42 â”‚      0.065  â”‚        2.7301 â”‚
â”‚ 2025-12-16 23:44:00 â”‚ 2025-12-16 23:45:00 â”‚ impression â”‚ FR      â”‚ banner        â”‚         103 â”‚      0.0    â”‚        0.0    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ë¶„ì„

**êµ­ê°€ë³„ ì´ ìˆ˜ìµ ìˆœìœ„:**

```sql
SELECT 
    country,
    sum(total_revenue) as total_revenue,
    sum(event_count) as total_events
FROM ad_events_aggregated
WHERE event_type = 'click'
GROUP BY country
ORDER BY total_revenue DESC;
```

**ê²°ê³¼:**

```
â”Œâ”€countryâ”€â”¬â”€â”€â”€â”€â”€â”€total_revenueâ”€â”¬â”€total_eventsâ”€â”
â”‚ US      â”‚            56.2412 â”‚          911 â”‚
â”‚ JP      â”‚            45.7469 â”‚          938 â”‚
â”‚ CA      â”‚            40.1562 â”‚          991 â”‚
â”‚ UK      â”‚            34.8419 â”‚          770 â”‚
â”‚ KR      â”‚            25.3185 â”‚          809 â”‚
â”‚ DE      â”‚            25.1154 â”‚          755 â”‚
â”‚ FR      â”‚            23.3569 â”‚          825 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì¸ì‚¬ì´íŠ¸:**

- ë¯¸êµ­(US)ì´ ê°€ì¥ ë†’ì€ eCPM â†’ ê°€ì¥ ë§ì€ ìˆ˜ìµ ìƒì„±
- ì„¤ì •í•œ êµ­ê°€ë³„ ê°€ì¤‘ì¹˜ê°€ ì •í™•íˆ ë°˜ì˜ë¨

**ê´‘ê³  í¬ë§·ë³„ ì„±ê³¼:**

```sql
SELECT 
    ad_format,
    sum(CASE WHEN event_type = 'impression' THEN event_count ELSE 0 END) as impressions,
    sum(CASE WHEN event_type = 'click' THEN event_count ELSE 0 END) as clicks,
    round(sum(total_revenue), 2) as revenue
FROM ad_events_aggregated
GROUP BY ad_format
ORDER BY revenue DESC;
```

**ê²°ê³¼:**

```
â”Œâ”€ad_formatâ”€â”€â”€â”€â”€â”€â”¬â”€impressionsâ”€â”¬â”€clicksâ”€â”¬â”€revenueâ”€â”
â”‚ rewarded_video â”‚       10937 â”‚   3554 â”‚  146.19 â”‚
â”‚ interstitial   â”‚       12427 â”‚   1705 â”‚   73.06 â”‚
â”‚ banner         â”‚       13636 â”‚    814 â”‚   34.92 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CTR ê³„ì‚°:**

- Rewarded Video: 3,554 / 10,937 = **32.5%** (ì„¤ì •ê°’ 25%)
- Interstitial: 1,705 / 12,427 = **13.7%** (ì„¤ì •ê°’ 12%)
- Banner: 814 / 13,636 = **6.0%** (ì„¤ì •ê°’ 5%)

â†’ **ì‹¤ì œ ê´‘ê³  ì‚°ì—…ì˜ CTR íŒ¨í„´ê³¼ ì¼ì¹˜!**

------

## ğŸ” Step 6: ì„±ëŠ¥ ë° ì•ˆì •ì„± ê²€ì¦

### 6.1 ì²˜ë¦¬ ì„±ëŠ¥

- **ì´ë²¤íŠ¸ ìƒì„± ì†ë„**: ~80 events/sec
- **Spark ì²˜ë¦¬ ì†ë„**: 10ì´ˆë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬
- **ClickHouse ì ì¬ ì§€ì—°**: < 1ì´ˆ
- **ì´ End-to-End ì§€ì—°**: ~10-15ì´ˆ

### 6.2 ë°ì´í„° ì •í™•ì„±

```sql
-- ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
SELECT 
    count(*) as total_records,
    count(DISTINCT window_start) as unique_windows,
    min(window_start) as first_window,
    max(window_start) as last_window
FROM ad_events_aggregated;
```

**ê²°ê³¼:**

- ì¤‘ë³µ ì—†ìŒ âœ…
- 1ë¶„ ë‹¨ìœ„ ìœˆë„ìš° ì •í™• âœ…
- ì‹œê°„ìˆœ ì •ë ¬ ì •ìƒ âœ…

### 6.3 WARNING ë©”ì‹œì§€ ë¶„ì„

ì‹¤í–‰ ì¤‘ ë°œìƒí•œ WARNING:

```
WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist
```

**ì´ìœ :**

- Spark Streamingì´ ì²˜ìŒ ì‹œì‘í•  ë•Œ ìƒíƒœ ì €ì¥ì†Œ ì´ˆê¸°í™”
- **ì •ìƒì ì¸ ë©”ì‹œì§€**ë¡œ, ì—ëŸ¬ê°€ ì•„ë‹˜

------

## ğŸ“ í•µì‹¬ í•™ìŠµ ë‚´ìš©

### 1. ClickHouse í…Œì´ë¸” ì„¤ê³„ Best Practices

**ORDER BY ì„ íƒ ê¸°ì¤€:**

- ê°€ì¥ ìì£¼ í•„í„°ë§ë˜ëŠ” ì»¬ëŸ¼ ìš°ì„ 
- Cardinalityê°€ ë†’ì€ ì»¬ëŸ¼ë¶€í„° ì •ë ¬
- ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ ì»¬ëŸ¼ ìš°ì„ 

**PARTITION BY ì „ëµ:**

- ë°ì´í„° ë³´ê´€ ì •ì±…ì— ë”°ë¼ ì„ íƒ (ì›”/ì¼/ì£¼)
- íŒŒí‹°ì…˜ í¬ê¸° 10GB~100GBê°€ ì ì •
- ë„ˆë¬´ ë§ì€ íŒŒí‹°ì…˜ì€ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜

### 2. Spark Streaming ClickHouse ì ì¬ íŒ¨í„´

**foreachBatch vs foreachPartition:**

- foreachBatch: ë§ˆì´í¬ë¡œë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ (ê¶Œì¥)
- foreachPartition: íŒŒí‹°ì…˜ ë‹¨ìœ„ ì²˜ë¦¬ (ì„¸ë°€í•œ ì œì–´ í•„ìš” ì‹œ)

**JDBC vs Native Protocol:**

- JDBC: ê°„ë‹¨í•˜ì§€ë§Œ ì„±ëŠ¥ ì œí•œì 
- Native (í–¥í›„ ê³ ë ¤): ê³ ì„±ëŠ¥, ë³µì¡í•œ ì„¤ì •

### 3. ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ë””ë²„ê¹…

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**

1. Kafka ì •ìƒ ì‹¤í–‰ í™•ì¸
2. Topic ë°ì´í„° í™•ì¸ (`kafka-console-consumer`)
3. Spark ë¡œê·¸ì—ì„œ ë°°ì¹˜ ì²˜ë¦¬ í™•ì¸
4. ClickHouseì— ë°ì´í„° ì ì¬ í™•ì¸
5. ì¿¼ë¦¬ë¡œ ë°ì´í„° ì •í•©ì„± ê²€ì¦

------

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (Day 4 Preview)

í˜„ì¬ê¹Œì§€ êµ¬ì¶•í•œ íŒŒì´í”„ë¼ì¸:

```
[Generator] â†’ [Kafka] â†’ [Spark] â†’ [ClickHouse] âœ…
```

**Day 4 ê³„íš:**

1. **Redis ìºì‹± ë ˆì´ì–´ ì¶”ê°€**
   - ì‹¤ì‹œê°„ ì¡°íšŒ ì„±ëŠ¥ ìµœì í™”
   - Hot data ìºì‹±
2. **ì¶”ê°€ ì§‘ê³„ ë¡œì§ êµ¬í˜„**
   - CTR (Click-Through Rate) ê³„ì‚°
   - eCPM (effective Cost Per Mille) ê³„ì‚°
   - ì‹¤ì‹œê°„ ì´ìƒ íƒì§€
3. **Superset ëŒ€ì‹œë³´ë“œ êµ¬ì¶•**
   - ClickHouse ì—°ë™
   - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì°¨íŠ¸

------

## ğŸ’¡ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ìš”ì•½

### ë¬¸ì œ 1: Kafka NodeExistsException

**ì›ì¸**: Zookeeperì— ë‚¨ì€ ë¸Œë¡œì»¤ ì •ë³´ **í•´ê²°**: ë°ì´í„° ì™„ì „ ì´ˆê¸°í™” í›„ ì¬ì‹œì‘

### ë¬¸ì œ 2: ClickHouse ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ë¯¸ì‹¤í–‰

**ì›ì¸**: ë°ì´í„°ë² ì´ìŠ¤ ë¨¼ì € ìƒì„± ì•ˆ í•¨ **í•´ê²°**: SQL íŒŒì¼ì— `CREATE DATABASE` ì¶”ê°€

### ë¬¸ì œ 3: Maven Dependency í˜•ì‹ ì˜¤ë¥˜

**ì›ì¸**: `:all` classifier ë¯¸ì§€ì› **í•´ê²°**: `--jars` ì˜µì…˜ìœ¼ë¡œ ì§ì ‘ JAR ì§€ì •

------

## ğŸ“ˆ ì„±ê³¼

âœ… **ì™„ì „í•œ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**

- Kafka â†’ Spark â†’ ClickHouse
- ì´ˆë‹¹ 80ê°œ ì´ë²¤íŠ¸ ì•ˆì •ì  ì²˜ë¦¬
- 10ì´ˆ ì´ë‚´ End-to-End ì§€ì—°

âœ… **í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ë°ì´í„° ëª¨ë¸ë§**

- íš¨ìœ¨ì ì¸ íŒŒí‹°ì…”ë‹ ì „ëµ
- ì¿¼ë¦¬ ìµœì í™”ëœ ì •ë ¬ í‚¤
- ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ë°˜ì˜

âœ… **ì‹¤ì‹œê°„ ë¶„ì„ ê¸°ëŠ¥**

- êµ­ê°€/í¬ë§·ë³„ ì„±ê³¼ ë¶„ì„
- CTR, eCPM ë“± AdTech í•µì‹¬ ì§€í‘œ
- SQL ê¸°ë°˜ ìœ ì—°í•œ ë¶„ì„

------

## ğŸ¯ í¬íŠ¸í´ë¦¬ì˜¤ í¬ì¸íŠ¸

ì´ í”„ë¡œì íŠ¸ì—ì„œ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ì—­ëŸ‰:

1. **ë¶„ì‚° ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì´í•´**
   - Kafka, Spark, ClickHouse í†µí•©
2. **ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ê²½í—˜**
   - Streaming ETL íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
3. **OLAP DB ìµœì í™”**
   - ClickHouse í…Œì´ë¸” ì„¤ê³„ ë° íŒŒí‹°ì…”ë‹
4. **íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ëŠ¥ë ¥**
   - Docker, Maven, JDBC ì´ìŠˆ í•´ê²°
5. **ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ì´í•´**
   - AdTech ë„ë©”ì¸ ì§€ì‹ ì ìš©

