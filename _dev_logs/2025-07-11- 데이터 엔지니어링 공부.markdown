---
layout: post
title: "데이터 엔지니어링 공부"
date: 2025-07-11
typora-root-url: ../
image_style: "max-width:80%; display:block; margin:1em auto; border-radius:10px; box-shadow:2px 2px 8px rgba(0,0,0,0.8);"
---





**추천 프로젝트: "실시간 이커머스 데이터 파이프라인"**

**1단계: 로컬 환경 구축**

- Docker Compose로 Hadoop, Spark, Kafka, PostgreSQL 환경 구축
- 가상의 이커머스 데이터 생성기 만들기 (주문, 결제, 사용자 행동 데이터)

**2단계: 실시간 스트리밍 파이프라인**

- Kafka로 실시간 주문/결제 이벤트 스트리밍
- Spark Streaming으로 실시간 집계 (분당 매출, 인기 상품 등)
- Redis에 실시간 대시보드용 데이터 저장

**3단계: 배치 처리 파이프라인**

- Airflow로 일배치 ETL 파이프라인 구축
- HDFS에 원본 데이터 저장
- Spark로 일별/월별 매출 분석, 사용자 세그먼트 분석

**4단계: 데이터 품질 관리**

- Great Expectations으로 데이터 검증 룰 설정
- 데이터 품질 모니터링 대시보드 구축

**5단계: 클라우드 확장**

- AWS/GCP로 환경 이전 (S3, BigQuery, EMR 등)
- Kubernetes로 컨테이너 오케스트레이션
- Terraform으로 인프라 코드화

**학습 순서 제안**

1. Docker + Kafka + Spark (1-2주)
2. Airflow + 배치 처리 (1-2주)
3. 데이터 품질 관리 (1주)
4. 클라우드 마이그레이션 (2-3주)







**AdTech 확장 시나리오**

**실시간 광고 입찰 시스템 (RTB)**

- 기존 이커머스 데이터에 광고 노출, 클릭, 전환 이벤트 추가
- Kafka로 실시간 입찰 요청/응답 스트리밍 (초당 수만건 처리)
- Redis/Aerospike로 사용자 프로필 실시간 조회 (1-2ms 응답)
- Spark Streaming으로 실시간 CTR/CVR 예측 모델 서빙

**어트리뷰션 분석 파이프라인**

- 멀티 터치포인트 추적 (노출→클릭→구매 경로)
- 복잡한 조인 처리 (시간 윈도우 기반 이벤트 매칭)
- 다양한 어트리뷰션 모델 구현 (First-click, Last-click, Linear 등)

**사용자 세그먼트 실시간 업데이트**

- 행동 데이터 기반 실시간 세그먼트 분류
- DMP(Data Management Platform) 구축
- 개인정보 보호 처리 (해싱, 익명화)

**확장 포인트들**

- **더 높은 처리량**: 초당 10만+ 이벤트 처리
- **더 낮은 지연시간**: 100ms 이내 실시간 응답
- **더 복잡한 분석**: 멀티 디멘션 집계, 머신러닝 파이프라인
- **개인정보 보호**: GDPR 준수 데이터 처리

**기술 스택 추가**

- ClickHouse (OLAP 분석용)
- Druid (실시간 분석용)
- Flink (더 정교한 스트림 처리)













## 📁 포트폴리오 예시 설계

### ✅ 1. **광고 로그 파이프라인 구축**

> **목표:** 광고 서버에서 발생한 click/impression 로그를 Kafka로 ingest하고, Spark로 집계 후 ClickHouse에 저장해 쿼리 가능한 형태로 제공

#### 구성

- `Kafka` + `Flask` → 광고 로그 전송
- `Spark Streaming` → 10초 단위 광고 지표 집계
- `ClickHouse` → 실시간 쿼리 저장소
- `Airflow` → 일 단위 ETL / 롤업 집계
- `Redash or Metabase` → CTR, CPC, ROI 시각화

#### 결과물

- GitHub 리포 + README 구성
- `광고 성과 지표 계산: CTR, CPC, CPM` 구현
- `dashboard screenshot` 첨부

------

### ✅ 2. **광고 추천 시스템 or 전환 예측 모델**

> **목표:** 간단한 lookalike targeting or 광고 전환율 예측 모델 제작

#### 구성

- 샘플 광고/유저 행동 데이터셋 (e.g., Criteo Dataset)
- Feature Engineering (session 기반, embedding)
- ML 모델: Logistic Regression, LightGBM 등
- 예측 결과 → Redis or FastAPI로 서비스화

#### 결과물

- 정확도 / AUC / PR curve
- 모델 비교: rule-based vs ML 모델
- inference API demo 제공

------

### ✅ 3. **실시간 리포팅 시스템**

> **목표:** 특정 캠페인의 실시간 클릭/전환/매출 데이터를 대시보드로 표현

#### 구성

- `Kafka → Flink or Spark Streaming → Redis`
- `Streamlit or Superset`으로 실시간 대시보드 구현
- 타임라인 기반 CTR/전환률 시각화
- KPI 알림 기능 (예: 전환률 급감 시 알림)

#### 결과물

- 실제 데이터로 리포팅 예시 시연 영상 or demo link
- 실시간성과 알림 로직에 대한 기술 설명 포함

------

## 🔥 포트폴리오 운영 방식

- GitHub + Notion 정리 → 이력서 링크로 활용
- README에 아키텍처 다이어그램, 기술스택, 목표, 핵심 구현 포인트 강조
- 모든 프로젝트는 **간단한 Streamlit 데모 페이지 or 서비스 형태**로 가능하면 제공
- 기술 블로그 2~3개 작성 (데이터 흐름, 광고 로그 분석기술, ML feature engineering 등)

------

## 🧭 다음 스텝 (구체적인 실행 순서)

1. **Kafka + Spark 기반 로그 파이프라인 실습** (2주)
2. **Criteo 등 퍼블릭 광고 데이터로 추천/예측 모델링** (2주)
3. **ClickHouse or Redis 기반 쿼리 서비스 실험** (1주)
4. **Streamlit/FastAPI로 대시보드, inference API 제공** (1주)
5. **모두 깃허브 정리 + 블로그 정리 + 시연 영상 제작** (2주)

→ 8주~10주 집중하면 **상당히 임팩트 있는 포트폴리오**가 만들어짐.

------

## 💬 혹시...

- 포트폴리오 예제용 데이터셋 추천?
- Kafka/Spark 실습 환경 셋업?
- GitHub 템플릿 or README 예시?
- 실제 이력서에 어떻게 녹여야 할지?

이 중 필요하면 바로 도와줄 수 있어. 뭐부터 시작할지 정해볼까?









## 핵심 전략: **실무 시나리오 + 오픈 데이터 + 유사 시스템 구현**

### ✅ 1. **"실제처럼 보이는" 광고 로그 만들기**

> ✅ 실무 경험이 없으니… 광고 로그를 *직접 만든다*
> (※ 절대 장난스럽지 않게, 진짜처럼 보이게 설계)

#### 🔧 방법:

- **유저 ID / 캠페인 ID / 타임스탬프 / 클릭여부 / 디바이스 / 지역** 등 실제 광고 로그 구조를 그대로 반영
- 10만~100만개 정도의 **가짜 로그 데이터 생성**
  - Python, Faker, pandas 등으로 생성 가능
  - 예: `log = {"user_id": "U123", "campaign_id": "C456", "click": 1, "ts": 2025-07-11 12:30}`

> → "가짜 로그"지만 **구조와 흐름은 실무와 동일**하므로 포트폴리오로 충분함

------

### ✅ 2. **"광고 로그 수집 → 처리 → 리포팅" 전체 흐름 구현**

> ✅ 실무와 동일한 아키텍처를 사용
> (모듈은 작게, 하지만 흐름은 진짜처럼)

#### 🔧 구성 예시:

```
csharp


복사편집
[1] 유저 클릭 로그 생성 (Faker + Python)  
 ↓
[2] Kafka로 로그 전송 (실시간 수집 흉내)  
 ↓
[3] Spark로 집계 (CTR, CVR 계산)  
 ↓
[4] ClickHouse에 저장 (빠른 쿼리)  
 ↓
[5] Superset/Redash로 실시간 리포팅
```

> → 이걸 구현하면 **실무 데이터 파이프라인 경험** 있다고 말해도 무방함.

------

### ✅ 3. **실제 광고 데이터셋을 활용한 모델링**

> ✅ 진짜 광고 데이터 기반의 예측 / 추천 시스템 구축
> (실제 기업들도 Criteo 데이터를 자주 씀)

#### 🔧 공개 데이터셋 추천

- 📦 Criteo Display Ads Dataset
- Avazu CTR Dataset (Kaggle)
- Outbrain Click Prediction

#### 🔧 구현 예시

- `광고 클릭 예측 (CTR)` 모델 만들기 (LightGBM, Logistic Regression)
- 예측 결과를 Redis에 저장 → FastAPI로 inference 서비스 제공

> → AI/ML 능력도 녹여낼 수 있음

------

### ✅ 4. **"실무형 README + 아키텍처 다이어그램"**

> ✅ 코드보다 중요한 건 **구조와 흐름을 설득력 있게 설명하는 문서화**

#### 필수 요소:

- 사용 기술 스택 요약
- 데이터 흐름 아키텍처 (다이어그램)
- 로그 예시 (샘플 10줄)
- 지표 정의 (CTR, CVR 등)
- 쿼리 결과 / 대시보드 캡처 이미지
- 개선 가능성 (ex. 배치 vs 스트리밍 비교, 성능 이슈 등)

> → 이것만 잘 써도 "이 사람 진짜 해본 사람"처럼 보임

------

## 💡 요약: 혼자서 실무급처럼 보이는 프로젝트 만들기

| 요소        | 방법                                  |
| ----------- | ------------------------------------- |
| 광고 로그   | Faker + Python으로 시뮬레이션         |
| 실시간 수집 | Kafka or 로그 파일 tailing            |
| 처리        | Spark or pandas ETL                   |
| 저장        | ClickHouse, PostgreSQL, BigQuery      |
| 리포팅      | Superset, Streamlit, Redash           |
| 모델링      | Criteo 데이터로 CTR 예측              |
| 문서화      | GitHub + 블로그 + 아키텍처 다이어그램 |



------

## 👊 나랑 같이 할 수 있어

원하면 이 모든 걸 **"단계별 프로젝트 가이드"**처럼 구성해서 도와줄 수도 있어.
예를 들어:

```
makefile


복사편집
1주차: 광고 로그 구조 만들기
2주차: Kafka로 스트리밍 구현
3주차: Spark로 지표 계산
4주차: ClickHouse/Redash로 리포팅
5주차: Criteo ML 모델 적용
6주차: 포트폴리오 정리
```



