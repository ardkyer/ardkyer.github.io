---
layout: post
title: "ë¸”ë¡œê·¸ ì¶”ì²œì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„  í”„ë¡œì íŠ¸2"
date: 2025-06-24
typora-root-url: ../
image_style: "max-width:80%; display:block; margin:1em auto; border-radius:10px; box-shadow:2px 2px 8px rgba(0,0,0,0.8);"
---





ìš°ì„  ë°ì´í„° ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë§Œë“¤ì–´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ .csvíŒŒì¼ë¡œ ë§Œë“¤ì–´ ì¶”ì¶œí–ˆìŒ. 20000ì¤„ ì •ë„ ë‚˜ì™”ìŒ.

ì²¨ì— .mdë§Œ ì¶”ì¶œí–‡ëŠ”ë° .mdë‘ .markdownì´ ì„ì—¬ìˆì–´ì„œ ë‹¤ì‹œ í†µí•©í•´ì„œ ì¶”ì¶œí•¨. 

```
python scripts/data_extraction.py --blog_path "../../ardkyer.github.io"                                              âœ” â”‚ base Py 
ğŸš€ ë°ì´í„° ì¶”ì¶œ ì‹œì‘ (.md + .markdown)
ğŸ“ _posts ì²˜ë¦¬ ì¤‘...
  ğŸ“„ íŒŒì¼ ìˆ˜: 83ê°œ
ğŸ“ _dev_logs ì²˜ë¦¬ ì¤‘...
  ğŸ“„ íŒŒì¼ ìˆ˜: 34ê°œ
ğŸ“ _further_reading ì²˜ë¦¬ ì¤‘...
  ğŸ“„ íŒŒì¼ ìˆ˜: 2ê°œ
ğŸ“ _paper_reviews ì²˜ë¦¬ ì¤‘...
  ğŸ“„ íŒŒì¼ ìˆ˜: 25ê°œ
\nğŸ“Š ì´ 144ê°œ íŒŒì¼ ë°œê²¬
\nâœ… 144ê°œ íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ!
\nğŸ“ ì»¬ë ‰ì…˜ë³„ ë¶„í¬:
  post: 83ê°œ (í‰ê·  463.6ë‹¨ì–´)
  dev_log: 34ê°œ (í‰ê·  690.1ë‹¨ì–´)
  further_reading: 2ê°œ (í‰ê·  54.5ë‹¨ì–´)
  paper_review: 25ê°œ (í‰ê·  647.9ë‹¨ì–´)
\nğŸ“„ íŒŒì¼ í™•ì¥ì ë¶„í¬:
  .md: 93ê°œ
  .markdown: 51ê°œ
\nğŸ“ ì €ì¥ ì™„ë£Œ: data/raw/all_posts_dataset.csv
\nğŸ” ìƒ˜í”Œ ë°ì´í„°:
                                     filename collection                        title  word_count
0  2024-12-17- FastAPIë¥¼ í™œìš©í•œ Online Serving.md       post  FastAPIë¥¼ í™œìš©í•œ Online Serving         308
1                       2024-10-29- ì¶”ì²œì‹œìŠ¤í…œ1.md       post                  ë² ì´ìŠ¤ë¼ì¸ ì˜¤í”¼ìŠ¤ì•„ì›Œ           5
2                2024-10-07- ê³µì›,í•™êµ í”¼ì²˜ ëŒë ¤ë³´ê¸°.md       post        ê³µì›, í•™êµ í”¼ì²˜ ë² ì´ìŠ¤ë¼ì¸ì— ë„£ì–´ë³´ê¸°         411
3                  2024-09-13- ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸.md       post                  ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸         624
4           2024-08-25-Week3 ì£¼ê°„ í•™ìŠµì •ë¦¬.markdown       post                Week3 ì£¼ê°„ í•™ìŠµì •ë¦¬          58
```







ìš°ì„  ê¸°ì¡´ì˜ ë¸”ë¡œê·¸ë“¤ì˜ ì¶”ì²œì‹œìŠ¤í…œ ì„±ëŠ¥ì§€í‘œë¥¼ í‰ê°€í•¨.

ê·¼ë° ì´ê±° ì–´ì¼€ ë‚˜ì˜¨ê±°ì§€? í•´ì„œ ì½”ë“œë¥¼ í•˜ë‚˜í•˜ë‚˜ì”© ì¢€ ëœ¯ì–´ë³´ê¸°ë¡œ í•¨.

```
python scripts/baseline_analysis.py                                                                                1 Ñ… â”‚ base Py 
ğŸš€ ë² ì´ìŠ¤ë¼ì¸ ì¶”ì²œì‹œìŠ¤í…œ ë¶„ì„ ì‹œì‘
============================================================
ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...
âœ… ê²Œì‹œë¬¼ ë°ì´í„°: 144ê°œ
âœ… ê¸°ì¡´ ì¶”ì²œ ë°ì´í„°: 65ê°œ ê²Œì‹œë¬¼

============================================================
ğŸ“‹ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ìš”ì•½
============================================================
ğŸ”¬ ë² ì´ìŠ¤ë¼ì¸ ê´€ë ¨ì„± ë¶„ì„
==================================================
í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: 0.105
ë¶„ì„ëœ ì¶”ì²œ ìˆ˜: 229ê°œ
ğŸ¯ ì¶”ì²œ ë‹¤ì–‘ì„± ë¶„ì„...
í‰ê·  ë‹¤ì–‘ì„± ì ìˆ˜: 0.225
ğŸ“ˆ ì¶”ì²œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„...
ì»¤ë²„ë¦¬ì§€: 0.174 (17.4%)
ğŸ” ì¶”ì²œ íŒ¨í„´ ë¶„ì„...
ê²Œì‹œë¬¼ë‹¹ í‰ê·  ì¶”ì²œ ìˆ˜: 4.6ê°œ
ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ì´ìœ : íƒœê·¸ì™€ ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ì„±...
ğŸ“Š ë°ì´í„° ìš”ì•½:
  - ì´ ê²Œì‹œë¬¼: 144ê°œ
  - ì¶”ì²œ ì‹œìŠ¤í…œ ì ìš© ê²Œì‹œë¬¼: 65ê°œ

ğŸ¯ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥:
  - ê´€ë ¨ì„± ì ìˆ˜: 0.105
  - ë‹¤ì–‘ì„± ì ìˆ˜: 0.225
  - ì»¤ë²„ë¦¬ì§€: 0.174

ğŸ“ˆ ì„±ëŠ¥ í‰ê°€:
  âŒ ê´€ë ¨ì„± ì ìˆ˜ê°€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (0.2 ë¯¸ë§Œ)
  âŒ ë‹¤ì–‘ì„±ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤
  âŒ ì»¤ë²„ë¦¬ì§€ê°€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤

ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: results/baseline/baseline_analysis.json

ğŸ‰ ë² ì´ìŠ¤ë¼ì¸ ë¶„ì„ ì™„ë£Œ!
ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì»¤ìŠ¤í…€ ML ëª¨ë¸ ê°œë°œë¡œ ì„±ëŠ¥ ê°œì„ 
ğŸ¯ ëª©í‘œ: ê´€ë ¨ì„± ì ìˆ˜ 0.105 â†’ 0.4+ (100%+ ê°œì„ )
```





```
BaselineAnalyzer í´ë˜ìŠ¤
 â”œâ”€ __init__         : íŒŒì¼ ê²½ë¡œ, ë°ì´í„° ì´ˆê¸°í™”
 â”œâ”€ load_data        : ê²Œì‹œë¬¼ CSV + ê¸°ì¡´ ì¶”ì²œ JSON ë¡œë“œ
 â”œâ”€ calculate_baseline_relevance : TF-IDF + cosine similarityë¡œ ê´€ë ¨ì„± ê³„ì‚°
 â”œâ”€ calculate_diversity_score    : ì¶”ì²œë“¤ì˜ collection ë‹¤ì–‘ì„± ì¸¡ì •
 â”œâ”€ calculate_coverage           : ì¶”ì²œëœ ê²Œì‹œë¬¼ë“¤ì´ ì „ì²´ datasetì˜ ëª‡ %ì¸ì§€
 â”œâ”€ analyze_recommendation_patterns : ì¶”ì²œ ê°¯ìˆ˜, ì´ìœ  í†µê³„ ë¶„ì„
 â”œâ”€ generate_summary_report      : ëª¨ë“  ì§€í‘œ ì¢…í•© + ì¶œë ¥ + ì €ì¥
 â””â”€ run_analysis                 : ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰

ë³´ì¡° í•¨ìˆ˜
 â””â”€ convert_numpy_types           : numpy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë°”ê¿” JSON ì €ì¥ ëŒ€ì‘

```





ê¸°ì¡´ related_posts.jsoníŒŒì¼ì„ ê°€ì ¸ì™€ì„œ í‰ê°€í•œë‹¤. ì—¬ê¸°ì„œ related_posts.jsonê°€ ë¬´ì—‡ì´ëƒë©´  

```
def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        self.df = pd.read_csv(self.data_path)
        print(f"âœ… ê²Œì‹œë¬¼ ë°ì´í„°: {len(self.df)}ê°œ")
        
        try:
            rec_path = os.path.join(self.blog_path, '_data', 'related_posts.json')
            with open(rec_path, 'r', encoding='utf-8') as f:
                self.existing_recs = json.load(f)
            print(f"âœ… ê¸°ì¡´ ì¶”ì²œ ë°ì´í„°: {len(self.existing_recs)}ê°œ ê²Œì‹œë¬¼")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ì¶”ì²œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.existing_recs = {}
```



ê¸°ì¡´ ì¶”ì²œì‹œìŠ¤í…œì´ êµ¬ì„±ë˜ëŠ” íŒŒì¼ì´ë‹¤. ëª‡ ê°œì›”ì „ì— ë§Œë“  íŒŒì¼ì¸ë° Claude APIë¥¼ í™œìš©í•´ ëª¨ë“  íŒŒì¼ë“¤ì„ ì „ë¶€ ë‹¤ ì½ê³  í•˜ë‚˜í•˜ë‚˜ ì²´í¬í•´ê°€ë©° ê°€ì¥ ìœ ì‚¬ë„ ë†’ì€ê±¸ 3ê°œì”© ì €ì¥í•´ì„œ ë‚˜ì˜¤ëŠ” êµ¬ì¡°ì˜€ë‹¤.

ìœ—ë¶€ë¶„ë§Œ ë´¤ì„ë•ŒëŠ” ê½¤ë‚˜ ì˜ ë‚˜ì˜¤ëŠ” ê²ƒ ê°™ì§€ë§Œ 

![image-20250624175938647](/assets/img/image-20250624175938647.png)

<br>



ì´ê²Œ Claude APIê°€ ìˆœì‹ê°„ì— ë¹¨ë ¤ê°€ì§€ê³  ì¤‘ê°„ë¶€í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‚°ì¶œë˜ëŠ” ë¬¸ì œê°€ ìˆì—ˆë‹¤. ì›ë˜ APIê°€ ë‹¤ ë‹³ìœ¼ë©´ ë‹¤ë¥¸ ëª¨ë¸ì´ë¼ë„ í´ë°± í•´ë†¨ì–´ì•¼ í•˜ëŠ”ë° APIë¹„ìš©ì´ ë¶€ì¡±í• ì§€ ìƒìƒí•˜ì§€ ëª»í•œ íƒ“ì´ë‹¤. ê·¸ê²ƒì´ ì„±ëŠ¥ì„ ì²´í¬í–ˆì„ë•Œ 0.1~0.2ë¥¼ ìƒíšŒí•˜ëŠ” ì´ìœ ê³ . ì‚¬ì‹¤ ì´ë•Œ ë‹¤ë¥¸ AI API ì—¬ëŸ¬ë°©ë©´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•´ë³¼ë•Œì˜€ëŠ”ë° Claude API ê°€ ê°œì¸ì ìœ¼ë¡œ ìµœì•…ì´ì—ˆë‹¤. ë§ë„ ì•ˆë˜ê²Œ ë¹„ì‹¸ë‹¤. ì•„ë§ˆ ì´ë²ˆì— APIëŠ” Geminaië¥¼ í™œìš©í• ê²ƒ ê°™ë‹¤.

![image-20250624180028057](/assets/img/image-20250624180028057.png)



ê¸°ì¡´ì½”ë“œ

```
#!/usr/bin/env python3
"""
ê°„ê²°í•œ ë² ì´ìŠ¤ë¼ì¸ ê´€ë ¨ì„± ì ìˆ˜ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import pandas as pd
import json
import numpy as np
import os
import argparse
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class SimpleRelevanceAnalyzer:
    def __init__(self, data_path='data/raw/all_posts_dataset.csv', 
                 blog_path='../../ardkyer.github.io'):
        self.data_path = data_path
        self.blog_path = blog_path
        self.df = None
        self.existing_recs = {}
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        self.df = pd.read_csv(self.data_path)
        print(f"âœ… ê²Œì‹œë¬¼ ë°ì´í„°: {len(self.df)}ê°œ")
        
        try:
            rec_path = os.path.join(self.blog_path, '_data', 'related_posts.json')
            with open(rec_path, 'r', encoding='utf-8') as f:
                self.existing_recs = json.load(f)
            print(f"âœ… ê¸°ì¡´ ì¶”ì²œ ë°ì´í„°: {len(self.existing_recs)}ê°œ ê²Œì‹œë¬¼")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ì¶”ì²œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.existing_recs = {}
    
    def calculate_relevance_score(self, sample_size=50):
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)"""
        print("ğŸ”¬ ê´€ë ¨ì„± ì ìˆ˜ ë¶„ì„ ì¤‘...")
        
        if not self.existing_recs:
            print("âŒ ì¶”ì²œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0.0
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(self.df['content'].fillna(''))
        
        relevance_scores = []
        
        for url, recs in list(self.existing_recs.items())[:sample_size]:
            # í˜„ì¬ í¬ìŠ¤íŠ¸ ì°¾ê¸°
            current_post_idx = None
            url_filename = url.split('/')[-1]
            
            for idx, row in self.df.iterrows():
                if (url_filename in row['filename'] or 
                    row['filename'].replace('.md', '').replace('.markdown', '') in url):
                    current_post_idx = idx
                    break
            
            if current_post_idx is None:
                continue
            
            # ê° ì¶”ì²œ í•­ëª©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            for rec in recs:
                rec_post_idx = None
                
                for idx, row in self.df.iterrows():
                    if rec['title'] in row['title'] or row['title'] in rec['title']:
                        rec_post_idx = idx
                        break
                
                if rec_post_idx is not None:
                    similarity = cosine_similarity(
                        tfidf_matrix[current_post_idx], 
                        tfidf_matrix[rec_post_idx]
                    )[0][0]
                    relevance_scores.append(similarity)
        
        if relevance_scores:
            avg_relevance = float(np.mean(relevance_scores))
            print(f"ğŸ“Š í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {avg_relevance:.3f}")
            print(f"ğŸ“ˆ ë¶„ì„ëœ ì¶”ì²œ ìˆ˜: {len(relevance_scores)}ê°œ")
            return avg_relevance
        else:
            print("âŒ ë¶„ì„ ê°€ëŠ¥í•œ ì¶”ì²œì´ ì—†ìŠµë‹ˆë‹¤")
            return 0.0
    
    def run_analysis(self):
        """ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ê´€ë ¨ì„± ì ìˆ˜ ë¶„ì„ ì‹œì‘")
        print("="*50)
        
        try:
            self.load_data()
            relevance_score = self.calculate_relevance_score()
            
            print("\n" + "="*50)
            print("ğŸ“‹ ë¶„ì„ ê²°ê³¼")
            print("="*50)
            print(f"ğŸ¯ ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.3f}")
            
            # ì„±ëŠ¥ í‰ê°€
            if relevance_score < 0.2:
                print("âŒ ê´€ë ¨ì„± ì ìˆ˜ê°€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (< 0.2)")
                improvement_needed = (0.4 - relevance_score) / relevance_score * 100
                print(f"ğŸ’¡ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ {improvement_needed:.0f}% ê°œì„  í•„ìš”")
            elif relevance_score < 0.4:
                print("âš ï¸ ê´€ë ¨ì„± ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ (< 0.4)")
                improvement_needed = (0.4 - relevance_score) / relevance_score * 100
                print(f"ğŸ’¡ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ {improvement_needed:.0f}% ê°œì„  í•„ìš”")
            else:
                print("âœ… ê´€ë ¨ì„± ì ìˆ˜ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤")
            
            print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
            return relevance_score
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

def main():
    parser = argparse.ArgumentParser(description="ê´€ë ¨ì„± ì ìˆ˜ ë¶„ì„")
    parser.add_argument('--data_path', default='data/raw/all_posts_dataset.csv')
    parser.add_argument('--blog_path', default='../../ardkyer.github.io')
    parser.add_argument('--sample_size', type=int, default=50, help='ë¶„ì„í•  ìƒ˜í”Œ í¬ê¸°')
    
    args = parser.parse_args()
    
    analyzer = SimpleRelevanceAnalyzer(args.data_path, args.blog_path)
    score = analyzer.run_analysis()
    
    return score

if __name__ == "__main__":
    main()
```



ì—¬ê¸°ì„œ

```
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
tfidf_matrix = vectorizer.fit_transform(self.df['content'].fillna(''))
```

TfidfVectorizerê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ì.



CountVectorizerë¥¼ í†µí•´ ìì—°ì–´ë¥¼ ë²¡í„°í™”í•˜ëŠ” ê²½ìš° ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œì (ì˜ë¯¸ ì—†ì´ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ì˜ ì¦ê°€ ë“±)ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ê°€ TfidfVectorizerë‹¤.

ê·¸ë ‡ë‹¤ë©´ CountVectorizerë€?

> **í…ìŠ¤íŠ¸(ë¬¸ì¥)ë¥¼ ë‹¨ì–´ì˜ ë“±ì¥ íšŸìˆ˜(Count)ë¡œ ë²¡í„°í™”**í•˜ëŠ” ë„êµ¬

![image-20250625153048242](/assets/img/image-20250625153048242.png)

ì•„ë§ˆ ì´ ì˜ˆì œë§Œ ë³´ë©´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì„ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤. 

ê° í–‰ = ë¬¸ì¥

ê° ì—´ = ë‹¨ì–´

ê°’ = **í•´ë‹¹ ë‹¨ì–´ê°€ ë¬¸ì¥ì— ë“±ì¥í•œ íšŸìˆ˜ (TF)**

------

## CountVectorizerì˜ ë¬¸ì œì 

### 1. ë„ˆë¬´ í”í•œ ë‹¨ì–´ê°€ ì¤‘ìš” ë‹¨ì–´ì²˜ëŸ¼ ì²˜ë¦¬ë¨

- ì˜ˆ: "the", "is", "and", "I", "you" ê°™ì€ ë‹¨ì–´ë“¤
- ë¬¸ë§¥ì—ëŠ” í° ì˜ë¯¸ ì—†ì§€ë§Œ, **ë¬¸ì„œì— ìì£¼ ë“±ì¥í•´ì„œ ë†’ì€ ê°€ì¤‘ì¹˜**ë¥¼ ê°–ê²Œ ë¨

### 2. ë‹¨ì–´ ì¤‘ìš”ë„ë¥¼ íŒë‹¨í•  ìˆ˜ ì—†ìŒ

- CountëŠ” ë‹¨ìˆœ íšŸìˆ˜ â†’ ì–´ë–¤ ë‹¨ì–´ê°€ **ì „ì²´ ë¬¸ì„œ ì¤‘ í¬ê·€í•˜ê±°ë‚˜ ì¤‘ìš”í•œì§€ íŒë‹¨ ëª»í•¨**

### 3. ë²¡í„° í¬ê¸°ê°€ ì»¤ì§ (í¬ì†Œ í–‰ë ¬ ë¬¸ì œ)

- ì „ì²´ ë§ë­‰ì¹˜ì— ë‹¨ì–´ê°€ 10,000ê°œë©´ â†’ ë²¡í„° ì°¨ì›ë„ 10,000



---



## í•´ê²°ì±… = **TfidfVectorizer**

> TF (ë‹¨ì–´ ë¹ˆë„) Ã— IDF (í¬ê·€ì„±) â†’ ë‹¨ì–´ì˜ **ì¤‘ìš”ë„**ë¥¼ ë°˜ì˜

- í”í•œ ë‹¨ì–´ â†’ ë‚®ì€ ì ìˆ˜
- í¬ê·€í•˜ê³  íŠ¹ì • ë¬¸ì„œì—ë§Œ ìì£¼ ì“°ì´ëŠ” ë‹¨ì–´ â†’ ë†’ì€ ì ìˆ˜

![image-20250625153254554](/assets/img/image-20250625153254554.png)

í•¨ìˆ˜ë§Œ ë°”ê¿§ë”ë‹ˆ ì´ë ‡ê²Œ ë‚˜ì˜¨ë‹¤. 

ì¢€ ë” ìˆ˜í•™ì ìœ¼ë¡œ êµ¬ì²´í™”í•˜ìë©´



```
corpus = [ 
    'you know I want your love', 
    'I like you', 
    'what should I do' 
]

ğŸ“Œ ë‹¨ì–´ë³„ IDF ê°’
do: 1.693
know: 1.693
like: 1.693
love: 1.693
should: 1.693
want: 1.693
what: 1.693
you: 1.288
your: 1.693

ğŸ“Œ ë¬¸ì„œë³„ TF (Term Frequency)
ë¬¸ì„œ 1: {'you': np.float64(0.16666666666666666), 'know': np.float64(0.16666666666666666), 'want': np.float64(0.16666666666666666), 'your': np.float64(0.16666666666666666), 'love': np.float64(0.16666666666666666), 'like': 0.0, 'what': 0.0, 'should': 0.0, 'do': 0.0}
ë¬¸ì„œ 2: {'you': np.float64(0.3333333333333333), 'know': 0.0, 'want': 0.0, 'your': 0.0, 'love': 0.0, 'like': np.float64(0.3333333333333333), 'what': 0.0, 'should': 0.0, 'do': 0.0}
ë¬¸ì„œ 3: {'you': 0.0, 'know': 0.0, 'want': 0.0, 'your': 0.0, 'love': 0.0, 'like': 0.0, 'what': np.float64(0.25), 'should': np.float64(0.25), 'do': np.float64(0.25)}

ğŸ“Œ ì •ê·œí™” ì „ TFÃ—IDF ë§¤íŠ¸ë¦­ìŠ¤
      do   know   like   love  should   want   what    you   your
0  0.000  1.693  0.000  1.693   0.000  1.693  0.000  1.288  1.693
1  0.000  0.000  1.693  0.000   0.000  0.000  0.000  1.288  0.000
2  1.693  0.000  0.000  0.000   1.693  0.000  1.693  0.000  0.000
you know I want your love

ğŸ“Œ L2 ì •ê·œí™”ëœ TFÃ—IDF ë§¤íŠ¸ë¦­ìŠ¤
      do   know   like   love  should   want   what    you   your
0  0.000  0.467  0.000  0.467   0.000  0.467  0.000  0.355  0.467
1  0.000  0.000  0.796  0.000   0.000  0.000  0.000  0.605  0.000
2  0.577  0.000  0.000  0.000   0.577  0.000  0.577  0.000  0.000
```

**cosine similarity, clustering ë“±ì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í• ìˆ˜ ìˆë„ë¡ TFÃ—IDF ë§¤íŠ¸ë¦­ìŠ¤ = (TF Ã— IDF) ì •ê·œí™”ëœ ê°’**

![image-20250625153826516](/assets/img/image-20250625153826516.png)



ìˆ˜í•™ì ìœ¼ë¡œëŠ” ë„ˆë¬´ ë“¤ì–´ê°€ì§€ ë§ì. ê·¸ë˜ì„œ ê°„ë‹¨íˆ ì •ë¦¬í•˜ìë©´

**TF (Term Frequency)**

- ë¬¸ì„œì—ì„œ ë‹¨ì–´ì˜ ìƒëŒ€ì  ë“±ì¥ ë¹„ìœ¨

**IDF (Inverse Document Frequency)**

- ì „ì²´ ë¬¸ì„œ ì¤‘ í¬ê·€í• ìˆ˜ë¡ IDF ë†’ìŒ

**TF-IDF = TF Ã— IDF**

- í”í•œ ë‹¨ì–´ â†’ ë‚®ì€ ì ìˆ˜
- í¬ê·€ ë‹¨ì–´ â†’ ë†’ì€ ì ìˆ˜

**L2 ì •ê·œí™”**

- ë²¡í„° ê¸¸ì´ë¥¼ 1ë¡œ ë§ì¶¤ â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”



ë‹¤ì‹œ ì´ì œ ì›ë˜ ì½”ë“œë¡œ ëŒì•„ì™€ì„œ GPTê°€ ì¨ì¤€ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•˜ë‹¤. ì—¬ê¸°ì„œ stop_words íŒŒë¼ë¯¸í„°ì˜ íš¨ìš©ì„±ì— ëŒ€í•´ ì°¾ì•„ë³´ì•˜ë‹¤. ì €ê²Œ ë¬´ì—‡ì´ê³  ì™œ ì“°ì´ëŠ”ì§€. 

```
# TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(self.df['content'].fillna(''))
```

[sklearn ê³µì‹ë¬¸ì„œ](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)

![image-20250625154303222](/assets/img/image-20250625154303222.png)

ì˜ì–´ë§Œ ë¶ˆìš©ì–´(at, is, ...) ë“±ì„ ì œê±°í•´ì£¼ëŠ”ê±° ê°™ë‹¤. ì•„ì§ì€ ì˜ì–´ë§Œ ì§€ì›í•˜ëŠ” ë“¯. ì‹¤ì œë¡œ í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›Œë„ ê´€ë ¨ì„± ì ìˆ˜ëŠ” ë³€í•˜ì§€ ì•Šì•˜ë‹¤.



https://foreverhappiness.tistory.com/30

ì•„ë§ˆ OKP, KoNLPyë“± í•œê¸€ ì „ìš©ì´ ë§ì€ ê±¸ë¡œ ì•Œê³ ì‡ëŠ”ë° ì´ê±´ ë‚˜ì¤‘ì— í•´ë³´ì. NLPê°•ì˜ë¥¼ ë‹¤ ë“¤ì–´ì•¼ëŒ€ë‚˜. ë‚˜ì¤‘ì— ê³µë¶€í•´ë³´ê³  ë°”ê¿”ì„œ ì ìš©í•´ë³¸ëŠ” ê²ƒë„ ì¬ë°Œì„ë“¯.



ì. ë‹¤ì‹œ ì½”ë“œë¡œ ëŒì•„ì™€ì„œ ìš°ë¦° ì§€ê¸ˆê¹Œì§€ ì´ í•¨ìˆ˜ì— ëŒ€í•´ ëœ¯ì–´ë³´ê³  ìˆì—ˆë‹¤. ì˜ˆì œë¥¼ í†µí•´ ì–´ì¼€ ëŒì•„ê°€ë‚˜ ì‚´í´ë³´ì.

```
def calculate_baseline_relevance(self, sample_size=50):
        """ë² ì´ìŠ¤ë¼ì¸ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
```

## ì˜ˆì œ 

**ë°ì´í„°í”„ë ˆì„ (self.df):**

```
idx  filename           title                    content
0    python-basics.md   "Python ê¸°ì´ˆ ë¬¸ë²•"        "ë³€ìˆ˜, í•¨ìˆ˜, í´ë˜ìŠ¤ì— ëŒ€í•´..."
1    django-tutorial.md "Django ì›¹ ê°œë°œ"          "ì›¹ í”„ë ˆì„ì›Œí¬ Djangoë¡œ..."
2    data-analysis.md   "ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ"         "pandas, numpyë¥¼ í™œìš©í•œ..."
3    machine-learning.md "ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸"           "scikit-learnì„ ì‚¬ìš©í•´ì„œ..."
```

**ê¸°ì¡´ ì¶”ì²œ ë°ì´í„° (related_posts.json):**

```
{
  "blog.com/python-basics": [
    {"title": "Django ì›¹ ê°œë°œ"},
    {"title": "ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ"}
  ],
  "blog.com/django-tutorial": [
    {"title": "Python ê¸°ì´ˆ ë¬¸ë²•"},
    {"title": "ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸"}
  ]
}
```

## ë‹¨ê³„ë³„ ì‹¤í–‰ ê³¼ì •

### 1ë‹¨ê³„: TF-IDF ë²¡í„°í™”

```python
# ê° contentë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜
tfidf_matrix = [
  [0.2, 0.8, 0.1, 0.0],  # python-basics.md ë²¡í„°
  [0.1, 0.3, 0.0, 0.9],  # django-tutorial.md ë²¡í„°
  [0.0, 0.4, 0.8, 0.2],  # data-analysis.md ë²¡í„°
  [0.3, 0.1, 0.6, 0.4]   # machine-learning.md ë²¡í„°
]
```

### 2ë‹¨ê³„: ì²« ë²ˆì§¸ URL ì²˜ë¦¬

```python
url = "blog.com/python-basics"
recs = [{"title": "Django ì›¹ ê°œë°œ"}, {"title": "ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ"}]

# URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
url_filename = "python-basics"  # url.split('/')[-1]

# í˜„ì¬ í¬ìŠ¤íŠ¸ ì°¾ê¸°
for idx, row in df.iterrows():
    if "python-basics" in row['filename']:  # python-basics.md
        current_post_idx = 0  # ì°¾ìŒ!
        break
```

### 3ë‹¨ê³„: ì²« ë²ˆì§¸ ì¶”ì²œ í•­ëª© ì²˜ë¦¬

```python
rec = {"title": "Django ì›¹ ê°œë°œ"}

# ì¶”ì²œ í¬ìŠ¤íŠ¸ ì°¾ê¸°
for idx, row in df.iterrows():
    if "Django ì›¹ ê°œë°œ" in row['title']:
        rec_post_idx = 1  # django-tutorial.md ì°¾ìŒ!
        break

# ìœ ì‚¬ë„ ê³„ì‚°
similarity = cosine_similarity(
    tfidf_matrix[0],  # python-basics.md ë²¡í„°
    tfidf_matrix[1]   # django-tutorial.md ë²¡í„°
)
# ê²°ê³¼: 0.234 (ì˜ˆì‹œ)
relevance_scores.append(0.234)
```

### 4ë‹¨ê³„: ë‘ ë²ˆì§¸ ì¶”ì²œ í•­ëª© ì²˜ë¦¬

```python
rec = {"title": "ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ"}

# ì¶”ì²œ í¬ìŠ¤íŠ¸ ì°¾ê¸° -> data-analysis.md (idx=2)
similarity = cosine_similarity(
    tfidf_matrix[0],  # python-basics.md ë²¡í„°
    tfidf_matrix[2]   # data-analysis.md ë²¡í„°
)
# ê²°ê³¼: 0.156 (ì˜ˆì‹œ)
relevance_scores.append(0.156)
```

### 5ë‹¨ê³„: ë‘ ë²ˆì§¸ URL ì²˜ë¦¬

```python
url = "blog.com/django-tutorial"
# ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬...
# Django -> Python ê¸°ì´ˆ: 0.234
# Django -> ë¨¸ì‹ ëŸ¬ë‹: 0.189
relevance_scores.append(0.234)
relevance_scores.append(0.189)
```

### 6ë‹¨ê³„: ìµœì¢… ê²°ê³¼

```python
relevance_scores = [0.234, 0.156, 0.234, 0.189]
baseline_relevance = np.mean(relevance_scores)  # 0.203

print(f"í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: 0.203")
print(f"ë¶„ì„ëœ ì¶”ì²œ ìˆ˜: 4ê°œ")
return 0.203
```



ì´ëŸ° ì‹ìœ¼ë¡œ ê¸°ì¡´ì˜ ì¶”ì²œì‹œìŠ¤í…œì„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•´ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°ì„ êµ¬í•œë‹¤.

<br>

---

## ê¸°ì¡´ ì„±ëŠ¥ ì§€í‘œ ì¸¡ì •ì™„ë£Œ

ì´ì œ ê¸°ì¡´ ë² ì´ìŠ¤ì˜ ì„±ëŠ¥ì„ ì¸¡ì •ì„ ì™„ë£Œí•˜ì˜€ìœ¼ë‹ˆ ê¸°ë³¸ ì¶”ì²œì‹œìŠ¤í…œ êµ¬ì¶•ì„ ë“¤ì–´ê°€ì.

```
BasicRecommendationGenerator í´ë˜ìŠ¤
â”œâ”€ load_data()                ë°ì´í„° ë¡œë”©
â”œâ”€ create_tfidf_vectors()     TF-IDF ë²¡í„°í™”
â”œâ”€ calculate_similarity()     ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
â”œâ”€ generate_recommendations() ê²Œì‹œë¬¼ë³„ ì¶”ì²œ ìƒì„±
â”œâ”€ save_recommendations()     JSON ì €ì¥ + í†µê³„
â”œâ”€ preview_recommendations()  ì¶œë ¥ ë¯¸ë¦¬ë³´ê¸°
â””â”€ run()                      ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

```



![image-20250625173717889](/assets/img/image-20250625173717889.png)



```
python scripts/simple_anal.py                                                                       âœ” â”‚ base Py 
ğŸš€ ê´€ë ¨ì„± ì ìˆ˜ ë¶„ì„ ì‹œì‘
==================================================
ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...
âœ… ê²Œì‹œë¬¼ ë°ì´í„°: 144ê°œ
âœ… ê¸°ì¡´ ì¶”ì²œ ë°ì´í„°: 144ê°œ ê²Œì‹œë¬¼ (v2 êµ¬ì¡°)
ğŸ”¬ ê´€ë ¨ì„± ì ìˆ˜ ë¶„ì„ ì¤‘...
ğŸ“Š í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: 0.304
ğŸ“ˆ ë¶„ì„ëœ ì¶”ì²œ ìˆ˜: 200ê°œ
ğŸ“ˆ ì¶”ì²œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì¤‘...
ğŸ“Š ì»¤ë²„ë¦¬ì§€: 0.924 (92.4%)
ğŸ“ˆ ì¶”ì²œëœ ê³ ìœ  ì½˜í…ì¸ : 133ê°œ / ì „ì²´ 144ê°œ

==================================================
ğŸ“‹ ë¶„ì„ ê²°ê³¼
==================================================
ğŸ¯ ê´€ë ¨ì„± ì ìˆ˜: 0.304
ğŸ“ˆ ì»¤ë²„ë¦¬ì§€: 0.924 (92.4%)

ğŸ“Š ì„±ëŠ¥ í‰ê°€:
âš ï¸ ê´€ë ¨ì„± ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ (< 0.4)
ğŸ’¡ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ 32% ê°œì„  í•„ìš”
âœ… ì»¤ë²„ë¦¬ì§€ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤

ğŸ‰ ë¶„ì„ ì™„ë£Œ!
```



| ì§€í‘œ        | ê¸°ì¡´  | ìƒˆë¡œë§Œë“  ê¸°ë³¸ ì¶”ì²œì‹œìŠ¤í…œ | ê°œì„ ìœ¨ | ë¹„ê³                   |
| ----------- | ----- | ------------------------ | ------ | --------------------- |
| ê´€ë ¨ì„± ì ìˆ˜ | 0.105 | 0.304                    | +189%  | ì¶”ì²œ í’ˆì§ˆ í–¥ìƒ        |
| ì»¤ë²„ë¦¬ì§€    | 17.4% | 92.4%                    | +431%  | ì¶”ì²œì´ ìƒì„±ëœ ë¬¸ì„œ ìˆ˜ |



ìš°ì„  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì ìˆ˜ëŠ” 189%ê°œì„ ë˜ì—ˆë‹¤. ì‚¬ì‹¤ìƒ ì›ë˜ ì¶”ì²œë°ì´í„°ê°€ ë„ˆë¬´ ì—‰ë§ì§„ì°½ì´ë¼ ë§ì´ ì˜¬ëë‹¤.

ì»¤ë²„ë¦¬ì§€ë€ ì „ì²´ ì½˜í…ì¸  ì¤‘ ëª‡ %ê°€ ì‹¤ì œë¡œ ì¶”ì²œë˜ì—ˆëŠ”ì§€ ê°’ì´ë‹¤. ì²˜ìŒì—ëŠ” ì¶”ì²œë˜ëŠ” ê²Œì‹œë¬¼ë“¤ë§Œ ì¶”ì²œë˜ë‹¤ ë³´ë‹ˆê¹Œ 17.4%ë¡œ êµ‰ì¥íˆ ì‘ì•„ì„œ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê°œì„ í•˜ë©´ì„œ ì»¤ë²„ë¦¬ì§€ë„ ì‹ ê²½ì¨ë³¼ê¹Œ í–ˆëŠ”ë° ìë™ì ìœ¼ë¡œ ìƒìŠ¹í–ˆë‹¤. ì•„ë¬´ë˜ë„ ì‘ì„±í•œ ê²Œì‹œë¬¼ë“¤ì´ ì¢€ ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì™€ ì£¼ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì“°ë‹¤ ë³´ë‹ˆ ì˜ í¼ì§€ê²Œ ë‚˜ì˜¨ ê²ƒ ê°™ë‹¤.

---

ì´ì œ ì„±ëŠ¥ì„ ì¡°ê¸ˆ ê°œì„ í•´ë³´ì. ì´ê²Œ ë­ Kaggleë„ ì•„ë‹ˆê³  ì—´ì¤‘í•  ê±´ ì—†ì§€ë§Œ ê°„ë‹¨íˆ íŠœë‹ì •ë„ëŠ” í•´ì£¼ëŠ”ê²Œ ì¢‹ì„ê²ƒê°™ë‹¤.

ì½”ë“œì—ì„œëŠ” ê·¸ë‚˜ë§ˆ ì‚´í´ë³¼ ê±´ ì´ê±°ê°™ë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” Gptê¸°ë³¸ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©í–ˆê³  ê³µì‹ë¬¸ì„œë¥¼ ë³´ë©´ì„œ ì„±ëŠ¥ì§€í‘œë¥¼ ê³„ì† ë³´ë©´ì„œ íŠœë‹ì„ í•œë²ˆ í•´ë³´ì.

```
# TF-IDF ë²¡í„°í™” (í•œêµ­ì–´ ê³ ë ¤)
        vectorizer = TfidfVectorizer(
            max_features=2000,          # íŠ¹ì„± ìˆ˜ ì¦ê°€
            stop_words='english',       # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°
            ngram_range=(1, 2),         # 1-gram, 2-gram ì‚¬ìš©
            min_df=2,                   # ìµœì†Œ 2ê°œ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë§Œ
            max_df=0.8                  # 80% ì´ìƒ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì œì™¸
        )
```

[sklearn ê³µì‹ë¬¸ì„œ](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)

íŒŒë¼ë¯¸í„°ê°€ ê¶ê¸ˆí•˜ë©´ ê³µì‹ë¬¸ì„œë¥¼ ì°¾ì•„ë³´ì.

---



## ë¬¸ì œ ì¸ì‹ ë° ë¶„ì„

### 1. ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ë¬¸ì œì 

ì´ˆê¸° ë¶„ì„ ê²°ê³¼ ê¸°ì¡´ ì¶”ì²œì‹œìŠ¤í…œì€ ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤:

```bash
ğŸ”¬ ë² ì´ìŠ¤ë¼ì¸ ê´€ë ¨ì„± ë¶„ì„
í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: 0.105
ë¶„ì„ëœ ì¶”ì²œ ìˆ˜: 229ê°œ
âŒ ê´€ë ¨ì„± ì ìˆ˜ê°€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (0.2 ë¯¸ë§Œ)
âŒ ì»¤ë²„ë¦¬ì§€ê°€ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (17.4%)
```

**ì£¼ìš” ë¬¸ì œì :**

- **API ì¤‘ë‹¨ìœ¼ë¡œ ì¸í•œ ë¶ˆì™„ì „í•œ ì¶”ì²œ ë°ì´í„°**
- **ë‹¨ìˆœ íƒœê·¸/ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë§¤ì¹­ì˜ í•œê³„**
- **ë¡±í…Œì¼ ì½˜í…ì¸  í™œìš©ë„ ë¶€ì¡±**

### 2. í‰ê°€ ë°©ë²•ë¡ ì˜ ì¤‘ìš”ì„±

í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ **ì˜¬ë°”ë¥¸ í‰ê°€ ë°©ë²•ë¡ **ì˜ ì¤‘ìš”ì„±ì„ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤:

#### âŒ ì˜ëª»ëœ í‰ê°€ ë°©ì‹ (Data Leakage)

```python
# ì „ì²´ ë°ì´í„°ë¡œ TF-IDF í•™ìŠµ
tfidf_matrix = vectorizer.fit_transform(all_posts)
# ê°™ì€ ë°ì´í„°ë¡œ í‰ê°€ â†’ ê³¼ì í•© ìœ„í—˜
similarity = cosine_similarity(post_A, post_B)
```

#### âœ… ì˜¬ë°”ë¥¸ í‰ê°€ ë°©ì‹ (Leave-One-Out)

```python
for target_post in all_posts:
    # íƒ€ê²Ÿ ì œì™¸í•œ ë‚˜ë¨¸ì§€ë¡œë§Œ í•™ìŠµ
    train_data = all_posts.drop(target_post)
    vectorizer.fit_transform(train_data)
    
    # íƒ€ê²Ÿì„ "ì²˜ìŒ ë³´ëŠ” ë°ì´í„°"ë¡œ í‰ê°€
    target_vector = vectorizer.transform([target_post])
    similarities = cosine_similarity(target_vector, train_matrix)
```

------

## ğŸ› ï¸ í•´ê²° ê³¼ì •

### 1ë‹¨ê³„: ê¸°ë³¸ TF-IDF ì¶”ì²œì‹œìŠ¤í…œ êµ¬ì¶•

```python
class BasicRecommendationGenerator:
    def create_tfidf_vectors(self):
        # ì œëª©ê³¼ ë‚´ìš© ê²°í•©ìœ¼ë¡œ ë” ì •í™•í•œ ìœ ì‚¬ë„ ê³„ì‚°
        combined_text = self.df['title'] + ' ' + self.df['content']
        
        vectorizer = TfidfVectorizer(
            max_features=2000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        
        self.tfidf_matrix = vectorizer.fit_transform(combined_text)
```

**ì´ˆê¸° ê²°ê³¼:**

- ê´€ë ¨ì„± ì ìˆ˜: 0.105 â†’ **0.304** (+189% ê°œì„ )
- ì»¤ë²„ë¦¬ì§€: 17.4% â†’ **92.4%** (+431% ê°œì„ )

### 2ë‹¨ê³„: TF-IDF íŒŒë¼ë¯¸í„° ìµœì í™”

ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤:

```python
# í…ŒìŠ¤íŠ¸í•œ ì£¼ìš” ì„¤ì •ë“¤
configs = {
    "ê¸°ë³¸ ì„¤ì •": {
        'max_features': 2000,
        'stop_words': 'english',
        'ngram_range': (1, 2),
        'min_df': 2,
        'max_df': 0.8
    },
    "í•œêµ­ì–´ ìµœì í™”": {
        'max_features': 2000,
        'stop_words': None,  # í•œêµ­ì–´ ë¸”ë¡œê·¸ì— ë” ì í•©
        'ngram_range': (1, 2),
        'min_df': 2,
        'max_df': 0.8
    }
}
```

**í•µì‹¬ ë°œê²¬ì‚¬í•­:**

- `stop_words=None`ì´ í•œêµ­ì–´ ë¸”ë¡œê·¸ì—ì„œ ë” íš¨ê³¼ì 
- ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°ê°€ ì˜¤íˆë ¤ í•œêµ­ì–´ ë§¥ë½ì„ í•´ì¹¨

### 3ë‹¨ê³„: ì˜¬ë°”ë¥¸ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•

ë°ì´í„° ëˆ„ìˆ˜ ì—†ëŠ” ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•´ Leave-One-Out í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì¶•:

```python
class ProperRecommendationEvaluator:
    def evaluate_tfidf_params_properly(self, tfidf_params):
        all_similarities = []
        
        # Leave-One-Out í‰ê°€
        for i in range(len(self.df)):
            # íƒ€ê²Ÿ ì œì™¸í•œ ë‚˜ë¨¸ì§€ë¡œ í•™ìŠµ
            train_indices = [j for j in range(len(df)) if j != i]
            train_matrix = vectorizer.fit_transform(train_data)
            
            # íƒ€ê²Ÿì„ ë¯¸ì§€ì˜ ë°ì´í„°ë¡œ ë³€í™˜
            target_vector = vectorizer.transform([target_text])
            
            # ìœ ì‚¬ë„ ê³„ì‚° ë° í‰ê°€
            similarities = cosine_similarity(target_vector, train_matrix)
```

### 4ë‹¨ê³„: ìµœì¢… ìµœì í™”

ì˜¬ë°”ë¥¸ í‰ê°€ ë°©ì‹ìœ¼ë¡œ ë‹¤ì‹œ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•œ ê²°ê³¼:

```python
ğŸ† ìµœì¢… ê²°ê³¼ ìˆœìœ„:
1ìœ„. 3-gram í¬í•¨: 0.3607
2ìœ„. í•œêµ­ì–´ ìµœì í™”: 0.3520
3ìœ„. ê¸°ë³¸ ì„¤ì •: 0.3502

ğŸ¥‡ ìµœì  íŒŒë¼ë¯¸í„°:
{
    'max_features': 2000,
    'stop_words': None,
    'ngram_range': (1, 3),  # 3-gramì´ ê²Œì„ ì²´ì¸ì €!
    'min_df': 2,
    'max_df': 0.8
}
```

------

## ğŸ¯ ìµœì¢… ì„±ê³¼

### ì •ëŸ‰ì  ì„±ê³¼

| ì§€í‘œ        | ê¸°ì¡´ ì‹œìŠ¤í…œ | â†’ ê°œì„ ìœ¨ | ê¸°ë³¸ ì¶”ì²œì‹œìŠ¤í…œ | â†’ ê°œì„ ìœ¨ | ê°œì„  í›„ ì‹œìŠ¤í…œ | ë¹„ê³                   |
| ----------- | ----------- | -------- | --------------- | -------- | -------------- | --------------------- |
| ê´€ë ¨ì„± ì ìˆ˜ | 0.105       | +189%    | 0.304           | +18.7%   | 0.3607         | ì¶”ì²œ í’ˆì§ˆ í–¥ìƒ        |
| ì»¤ë²„ë¦¬ì§€    | 17.4%       | +431%    | 92.4%           | +0%      | 92.4%          | ì¶”ì²œì´ ìƒì„±ëœ ë¬¸ì„œ ìˆ˜ |

### ì§ˆì  ê°œì„ ì‚¬í•­

1. **ì•ˆì •ì„± í™•ë³´**: ëª¨ë“  144ê°œ ê²Œì‹œë¬¼ì— ì¼ê´€ëœ ì¶”ì²œ ì œê³µ
2. **ë‹¤ì–‘ì„± ì¦ëŒ€**: 92.4% ì»¤ë²„ë¦¬ì§€ë¡œ ë¡±í…Œì¼ ì½˜í…ì¸  í™œìš©
3. **ê´€ë ¨ì„± í–¥ìƒ**: 0.105 â†’ 0.3607ë¡œ ëŒ€í­ ê°œì„ 

------

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. í‰ê°€ ë°©ë²•ë¡ ì˜ ì¤‘ìš”ì„±

- **Data Leakage ë°©ì§€**ê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì¸¡ì •ì˜ í•µì‹¬
- **Leave-One-Out ë°©ì‹**ìœ¼ë¡œ ì‹¤ì œ ìš´ì˜ í™˜ê²½ê³¼ ë™ì¼í•œ ì¡°ê±´ í‰ê°€

### 2. í•œêµ­ì–´ ë¸”ë¡œê·¸ íŠ¹ì„± ê³ ë ¤

- `stop_words=None`ì´ í•œêµ­ì–´ ì½˜í…ì¸ ì—ì„œ ë” íš¨ê³¼ì 
- ì˜ì–´ ì¤‘ì‹¬ NLP ë„êµ¬ì˜ í•œê³„ ì¸ì‹ í•„ìš”

### 3. N-gramì˜ ìœ„ë ¥

- **3-gram í¬í•¨**ìœ¼ë¡œ ë” ì •í™•í•œ ë¬¸ë§¥ ì´í•´
- ë‹¨ìˆœ ë‹¨ì–´ ë§¤ì¹­ì„ ë„˜ì–´ì„  êµ¬ë¬¸ íŒ¨í„´ ì¸ì‹

### 4. ì»¤ë²„ë¦¬ì§€ì˜ ì¤‘ìš”ì„±

- **92.4% ì»¤ë²„ë¦¬ì§€**ë¡œ í¸í–¥ ì—†ëŠ” ì¶”ì²œ ë‹¬ì„±
- ì¸ê¸° ì½˜í…ì¸ ë¿ë§Œ ì•„ë‹ˆë¼ ë¡±í…Œì¼ ì½˜í…ì¸ ë„ ê³¨ê³ ë£¨ ì¶”ì²œ



### í•µì‹¬ ì•Œê³ ë¦¬ì¦˜

```python
# ìµœì í™”ëœ TF-IDF ì„¤ì •
vectorizer = TfidfVectorizer(
    max_features=2000,      # ì ì • ì–´íœ˜ í¬ê¸°
    stop_words=None,        # í•œêµ­ì–´ ìµœì í™”
    ngram_range=(1, 3),     # 3-gram í¬í•¨
    min_df=2,               # ë…¸ì´ì¦ˆ ì œê±°
    max_df=0.8              # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
)

# Leave-One-Out í‰ê°€
for each_post in all_posts:
    exclude_target_from_training()
    train_model_on_remaining_data()
    evaluate_on_target_post()
```



![image-20250625194259701](/assets/img/image-20250625194259701.png)

ê¹”ë”í•˜ê²Œ ë¸”ë¡œê·¸ì— ì ìš©ê¹Œì§€ ì™„ë£Œí•˜ì˜€ë‹¤. ì´ì œì•¼ ì¢€ ì˜ ë‚˜ì˜¤ë„¤. í”„ë¡ íŠ¸ì—”ë“œ í•˜ë©´ì„œ íŒŒì¼í˜•ì‹ë“¤ì´ ë‹¤ ì œë©‹ëŒ€ë¡œë“¤ì´ë¼ í˜ë“¤ì—ˆì§€ë§Œ ìƒëµí•œë‹¤



---



## ë‹¤ìŒ ë‹¨ê³„ ê³„íš

### Phase 2: LLM ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ

- **Gemini API** í™œìš©í•œ ì˜ë¯¸ì  ì´í•´ ê¸°ë°˜ ì¶”ì²œ
- **í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸**: TF-IDF + LLM ì•™ìƒë¸”

### Phase 3: MLOps íŒŒì´í”„ë¼ì¸

- **Apache Airflow**ë¥¼ í™œìš©í•œ ìë™í™” íŒŒì´í”„ë¼ì¸
- **A/B í…ŒìŠ¤íŠ¸** í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
- **ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**



------

## ğŸ‰ ê²°ë¡ 

ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ **244% ì„±ëŠ¥ í–¥ìƒ**ì´ë¼ëŠ” ê´„ëª©í•  ë§Œí•œ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ **ì˜¬ë°”ë¥¸ í‰ê°€ ë°©ë²•ë¡ **ì˜ ì¤‘ìš”ì„±ì„ ê¹¨ë‹«ê³  ì´ë¥¼ ì‹¤ì œë¡œ êµ¬í˜„í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì¸¡ì •ì„ í–ˆë‹¤ëŠ” ì ì´ ê°€ì¥ í° ì„±ê³¼ì…ë‹ˆë‹¤.

ë‹¨ìˆœí•œ ì„±ëŠ¥ ê°œì„ ì„ ë„˜ì–´ì„œ **ì²´ê³„ì ì¸ ML í”„ë¡œì íŠ¸ ìˆ˜í–‰ ëŠ¥ë ¥**ê³¼ **ì •í™•í•œ í‰ê°€ ë°©ë²•ë¡  ì´í•´**ë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” í”„ë¡œì íŠ¸ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” LLMì„ í™œìš©í•œ ë” ê³ ë„í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „ì‹œì¼œ ë‚˜ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤! ğŸš€

------





















