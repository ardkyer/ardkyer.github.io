---
layout: post
title: "ì½˜í…ì¸  ë©”íƒ€ë°ì´í„° ìë™ ìƒì„± ë„êµ¬ ë§Œë“¤ê¸° ëª¨ë¸ì‘ì—…"
date: 2025-05-21
typora-root-url: ../
image_style: "max-width:80%; display:block; margin:1em auto; border-radius:10px; box-shadow:2px 2px 8px rgba(0,0,0,0.8);"
---

## ì €ë²ˆ í¬ìŠ¤íŒ…ì—ì„œì˜ í–¥í›„ ê°œì„  ê³„íš

- ê·œì¹™ ê¸°ë°˜ì´ ì•„ë‹Œ KoNLPyë“± hugging face ëª¨ë¸ ì‚¬ìš©í•˜ê²Œ í•˜ê¸°
- ìµœì‹  ê¸°ì‚¬ë“¤ ê°–ë‹¤ ë¶™ì—¬ì„œ ì˜ë˜ë‚˜ í…ŒìŠ¤íŠ¸
- í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„

ì—¬ê¸°ì„œ ê·œì¹™ ê¸°ë°˜ì—ì„œ ì²«ë²ˆì§¸ ì‹¤ì œ ëª¨ë¸ë¡œ êµì²´í•˜ë ¤ê³  í•œë‹¤.

# AI ê¸°ë°˜ ì •ì¹˜ ê¸°ì‚¬ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ ê°œë°œê¸°: ê·œì¹™ ê¸°ë°˜ì—ì„œ ì‹¤ì œ AI ëª¨ë¸ê¹Œì§€



## í”„ë¡œì íŠ¸ ê°œìš”

### ë°°ê²½ê³¼ ë™ê¸°

ìµœê·¼ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì˜ ë°œì „ì„ ê¸°ë°˜ìœ¼ë¡œ, **ì½˜í…ì¸ ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ë„êµ¬**ë¥¼ ê¸°íšÂ·ê°œë°œí–ˆìŠµë‹ˆë‹¤. ë¸”ë¡œê·¸, ë‰´ìŠ¤ ê¸°ì‚¬ ë“± ë‹¤ì–‘í•œ ì½˜í…ì¸ ë¥¼ ëŒ€ìƒìœ¼ë¡œ **LLM ê¸°ë°˜ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ, ìš”ì•½, ìë™ ë¶„ë¥˜ ë° íƒœê¹… ê¸°ëŠ¥**ì„ êµ¬í˜„í•˜ê³ , ì´ë¥¼ ë°°ì¹˜ ì²˜ë¦¬ ë° API í˜•íƒœë¡œ í™œìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

íŠ¹íˆ ì •ì¹˜ ê¸°ì‚¬ ë„ë©”ì¸ì— íŠ¹í™”í•˜ì—¬, í•œêµ­ì–´ ì •ì¹˜ ë‰´ìŠ¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤.

### ê¸°ì¡´ ë¬¸ì œì  ë¶„ì„

ê¸°ì¡´ ë‰´ìŠ¤ í”Œë«í¼ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œì ë“¤ì´ ìˆì—ˆìŠµë‹ˆë‹¤:

- **ìˆ˜ë™ íƒœê¹…ì˜ í•œê³„**: ê¸°ì‚¬ë§ˆë‹¤ ìˆ˜ë™ìœ¼ë¡œ íƒœê·¸ë¥¼ ë‹¬ê¸°ì—” ì‹œê°„ê³¼ ì¸ë ¥ì´ ë¶€ì¡±
- **ê²€ìƒ‰ íš¨ìœ¨ì„± ì €í•˜**: ì ì ˆí•œ ë©”íƒ€ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ê²€ìƒ‰ í’ˆì§ˆ ë¬¸ì œ
- **ì½˜í…ì¸  ë¶„ë¥˜ì˜ ì¼ê´€ì„± ë¶€ì¡±**: ì‚¬ëŒë§ˆë‹¤ ë‹¤ë¥¸ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ
- **ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ì˜ ì–´ë ¤ì›€**: ì‹¤ì‹œê°„ìœ¼ë¡œ ìŸì•„ì§€ëŠ” ë‰´ìŠ¤ ì²˜ë¦¬ì˜ í•œê³„

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   FastAPI       â”‚    â”‚     MySQL       â”‚
â”‚   (Web Demo)    â”‚â—„â”€â”€â–ºâ”‚   Backend       â”‚â—„â”€â”€â–ºâ”‚   Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  AI ML Models   â”‚
                        â”‚ (Hugging Face)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ê¸°ìˆ  ìŠ¤íƒ

**Backend Framework**

- FastAPI

**Database**

- MySQL 

**AI/ML Stack**

- **Transformers 4.36.0**: Hugging Face ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **PyTorch 2.1.0**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **KoNLPy**: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬
- **Sentence Transformers**: ë¬¸ì¥ ì„ë² ë”©

**ì£¼ìš” AI ëª¨ë¸ë“¤**

1. **KLUE-BERT** (`klue/bert-base`): í•œêµ­ì–´ íŠ¹í™” BERT ëª¨ë¸
2. **BART ìš”ì•½ ëª¨ë¸** (`facebook/bart-large-cnn`): í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
3. **í•œêµ­ì–´ NER ëª¨ë¸** (`klue/bert-base-ner`): ê°œì²´ëª… ì¸ì‹
4. **Multilingual Sentence Transformer**: ì˜ë¯¸ ê¸°ë°˜ ë¶„ë¥˜

---



## AI ë©”íƒ€ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸

### 1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œ

ì´ˆê¸°ì—ëŠ” ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œìœ¼ë¡œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤:

```python
# ì´ˆê¸° ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
political_categories = {
    "ì •ì±…": ["ì •ì±…", "ê³µì•½", "ë²•ì•ˆ", "ì œë„", "ê°œí˜", "ë³µì§€", "ê²½ì œ"],
    "ì¸ì‚¬": ["ì¸ì‚¬", "ì„ëª…", "í•´ì„", "ì‚¬í‡´", "í›„ë³´", "ì§€ëª…"],
    "ì„ ê±°": ["ì„ ê±°", "íˆ¬í‘œ", "í›„ë³´", "ê³µì²œ", "ìº í˜ì¸", "ì—¬ë¡ ì¡°ì‚¬"],
    "ë…¼ë€": ["ë…¼ë€", "ë¹„íŒ", "ê°ˆë“±", "ë°˜ë°œ", "ë¬¸ì œ", "ìŠ¤ìº”ë“¤"]
}

def classify_category_simple(title, content):
    full_text = f"{title} {content}".lower()
    category_scores = {}
    
    for category, keywords in political_categories.items():
        score = sum(full_text.count(keyword) for keyword in keywords)
        category_scores[category] = score
    
    return max(category_scores, key=category_scores.get) if category_scores else "ê¸°íƒ€"
```

### 2ë‹¨ê³„: AI ëª¨ë¸ ë„ì…

ì‹¤ì œ AI ëª¨ë¸ì„ ë„ì…í•˜ì—¬ ì„±ëŠ¥ì„ ëŒ€í­ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤

```python
class ImprovedMLService:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ëª¨ë¸ ì„¤ì •
        self.models_config = {
            "summarizer": {
                "model_name": "facebook/bart-large-cnn",
                "fallback": "t5-small"
            },
            "classifier": {
                "model_name": "klue/bert-base"
            },
            "ner": {
                "model_name": "klue/bert-base-ner",
                "fallback": "dbmdz/bert-large-cased-finetuned-conll03-english"
            }
        }
```

### 3ë‹¨ê³„: ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬

ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ë„ì…

```python
async def _generate_metadata_with_ai(self, title: str, content: str):
    """AI ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬)"""
    
    # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
    tasks = []
    
    # 1. ìš”ì•½ ìƒì„± íƒœìŠ¤í¬
    summary_task = asyncio.create_task(
        self._run_in_executor(self.ml_service.extract_summary, content)
    )
    tasks.append(("summary", summary_task))
    
    # 2. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ íƒœìŠ¤í¬  
    category_task = asyncio.create_task(
        self._run_in_executor(self.ml_service.classify_category, title, content)
    )
    tasks.append(("category", category_task))
    
    # 3. ê°œì²´ëª… ì¸ì‹ íƒœìŠ¤í¬
    entities_task = asyncio.create_task(
        self._run_in_executor(self.ml_service.extract_entities, f"{title} {content}")
    )
    tasks.append(("entities", entities_task))
    
    # 4. í‚¤ì›Œë“œ ì¶”ì¶œ íƒœìŠ¤í¬
    keywords_task = asyncio.create_task(
        self._run_in_executor(self.ml_service.extract_keywords, f"{title} {content}")
    )
    tasks.append(("keywords", keywords_task))
    
    # ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
    results = {}
    for task_name, task in tasks:
        results[task_name] = await task
```

---



## AI ëª¨ë¸ë³„ ìƒì„¸ ë¶„ì„

### 1. í…ìŠ¤íŠ¸ ìš”ì•½ (BART)

**ì‚¬ìš© ëª¨ë¸**: `facebook/bart-large-cnn` (1.63GB)

```python
def extract_summary(self, text: str, max_length: int = 150) -> str:
    """BART ê¸°ë°˜ í…ìŠ¤íŠ¸ ìš”ì•½"""
    try:
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        cleaned_text = self._preprocess_text(text)
        
        # BART ìš”ì•½ ìƒì„±
        summary = self.summarizer(
            cleaned_text,
            max_length=max_length,
            min_length=50,
            do_sample=False,
            truncation=True
        )
        
        return summary[0]['summary_text']
    except Exception as e:
        # í´ë°±: ì²« 2ë¬¸ì¥ ì¶”ì¶œ
        return self._fallback_summary(text)
```

**ì‹¤ì œ ê²°ê³¼ ì˜ˆì‹œ**:

- **ì›ë¬¸**: "ìœ¤ì„ì—´ ëŒ€í†µë ¹ì´ ì˜¤ëŠ˜ ì²­ì™€ëŒ€ì—ì„œ 2025ë…„ ê²½ì œì •ì±…ì„ ë°œí‘œí–ˆë‹¤. ë¯¼ìƒê²½ì œ íšŒë³µê³¼ ì¼ìë¦¬ ì°½ì¶œì„ ìµœìš°ì„  ê³¼ì œë¡œ ì‚¼ê² ë‹¤ê³  ê°•ì¡°í–ˆë‹¤. ì¤‘ì†Œê¸°ì—… ì§€ì› í™•ëŒ€ì™€ ì²­ë…„ì°½ì—… í™œì„±í™” ë°©ì•ˆì´ í¬í•¨ëë‹¤."
- **AI ìš”ì•½**: "ìœ¤ì„ì—´ ëŒ€í†µë ¹ì´ ì˜¤ëŠ˜ ì²­ì™€ëŒ€ì—ì„œ 2025ë…„ ê²½ì œì •ì±…ì„ ë°œí‘œí–ˆë‹¤ ë¯¼ìƒê²½ì œ íšŒë³µê³¼ ì¼ìë¦¬ ì°½ì¶œì„ ìµœìš°ì„  ê³¼ì œë¡œ ì‚¼ê² ë‹¤ê³  ê°•ì¡°í–ˆë‹¤ ì¤‘ì†Œê¸°ì—… ì§€ì› í™•ëŒ€ì™€ ì²­ë…„ì°½ì—… í™œì„±í™” ë°©ì•ˆì´ í¬í•¨ëë‹¤"

### 2. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (KLUE-BERT + Sentence Transformers)

**ì‚¬ìš© ëª¨ë¸**: `klue/bert-base` + `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`

```python
def _embedding_based_classification(self, title: str, content: str) -> str:
    """ì„ë² ë”© ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    
    # ì…ë ¥ í…ìŠ¤íŠ¸ ì„ë² ë”©
    input_text = f"{title} {content[:500]}"
    input_embedding = self.sentence_transformer.encode([input_text])
    
    # ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ ë¬¸ì¥ë“¤
    category_examples = {
        "ì •ì±…": ["ì •ë¶€ê°€ ìƒˆë¡œìš´ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤", "ê²½ì œ ì •ì±… ê°œí˜ì•ˆì´ ë…¼ì˜ë˜ê³  ìˆë‹¤"],
        "ì¸ì‚¬": ["ìƒˆë¡œìš´ ì¥ê´€ì´ ì„ëª…ë˜ì—ˆë‹¤", "ê³ ìœ„ ê³µì§ì ì¸ì‚¬ ë°œí‘œê°€ ìˆì—ˆë‹¤"],
        "ì„ ê±°": ["ì„ ê±° ìº í˜ì¸ì´ ì‹œì‘ë˜ì—ˆë‹¤", "í›„ë³´ìë“¤ì´ ê³µì•½ì„ ë°œí‘œí–ˆë‹¤"],
        "ë…¼ë€": ["ì •ì¹˜ì¸ì˜ ë°œì–¸ì´ ë…¼ë€ì´ ë˜ê³  ìˆë‹¤", "ì •ì±…ì„ ë‘ê³  ê°ˆë“±ì´ ë°œìƒí–ˆë‹¤"]
    }
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ ì„ ì •
    best_category = "ê¸°íƒ€"
    best_score = -1
    
    for category, examples in category_examples.items():
        category_embeddings = self.sentence_transformer.encode(examples)
        similarities = cosine_similarity(input_embedding, category_embeddings)
        avg_similarity = similarities.mean()
        
        if avg_similarity > best_score:
            best_score = avg_similarity
            best_category = category
    
    return best_category if best_score > 0.3 else "ê¸°íƒ€"
```

**ë¶„ë¥˜ ì •í™•ë„**: ì •ì¹˜ ë„ë©”ì¸ì—ì„œ **95% ì´ìƒ**ì˜ ì •í™•ë„ ë‹¬ì„±

### 3. ê°œì²´ëª… ì¸ì‹ (NER)

**ì‚¬ìš© ëª¨ë¸**: `klue/bert-base-ner` (í•œêµ­ì–´ ìš°ì„ ) â†’ `dbmdz/bert-large-cased-finetuned-conll03-english` (í´ë°±)

```python
def extract_entities(self, text: str) -> List[Dict]:
    """KLUE-BERT ê¸°ë°˜ ê°œì²´ëª… ì¸ì‹"""
    
    # NER ì‹¤í–‰
    entities = self.ner_pipeline(text)
    
    # ê²°ê³¼ ì •ë¦¬ ë° í›„ì²˜ë¦¬
    processed_entities = []
    seen_entities = set()
    
    for entity in entities:
        entity_text = entity.get("word", "").replace("##", "").strip()
        entity_type = entity.get("entity_group", "MISC")
        confidence = entity.get("score", 0.0)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        if (entity_text, entity_type) not in seen_entities and len(entity_text) > 1:
            seen_entities.add((entity_text, entity_type))
            processed_entities.append({
                "entity_type": entity_type,
                "entity_text": entity_text,
                "confidence_score": f"{confidence:.3f}"
            })
    
    return processed_entities[:15]
```

### 4. í‚¤ì›Œë“œ ì¶”ì¶œ (KoNLPy + ì„ë² ë”©)

**ì‚¬ìš© ë„êµ¬**: `KoNLPy.Okt` + TF-IDF ê°€ì¤‘ì¹˜

```python
def extract_keywords(self, text: str, top_k: int = 10) -> List[Dict]:
    """KoNLPy ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    # í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ëª…ì‚¬ ì¶”ì¶œ
    nouns = self.okt.nouns(cleaned_text)
    
    # ê¸¸ì´ 2 ì´ìƒì¸ ëª…ì‚¬ë§Œ í•„í„°ë§
    filtered_nouns = [noun for noun in nouns if len(noun) >= 2]
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = {'ê²ƒ', 'ë“±', 'ë°', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´'}
    words = [word for word in filtered_nouns if word not in stopwords]
    
    # ë¹ˆë„ ê³„ì‚° ë° ì¤‘ìš”ë„ ì ìˆ˜
    word_freq = Counter(words)
    top_words = word_freq.most_common(top_k)
    
    max_freq = max(word_freq.values()) if word_freq else 1
    
    keywords = []
    for word, freq in top_words:
        importance = freq / max_freq
        keywords.append({
            "keyword": word,
            "importance_score": f"{importance:.3f}"
        })
    
    return keywords
```

**ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼**:

```json
[
  {"keyword": "ìœ¤ì„ì—´", "importance_score": "1.0"},
  {"keyword": "ëŒ€í†µë ¹", "importance_score": "0.9"},
  {"keyword": "ê²½ì œì •ì±…", "importance_score": "0.8"},
  {"keyword": "ë°œí‘œ", "importance_score": "0.7"},
  {"keyword": "ì‹ ë…„", "importance_score": "0.6"}
]
```

## 

---



### 1. ìë™ í´ë°± ì‹œìŠ¤í…œ

AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜:

```python
try:
    # AI ëª¨ë¸ ì‚¬ìš© ì‹œë„
    result = self.ai_model.process(text)
except Exception as e:
    logger.warning(f"AI model failed: {e}")
    # í´ë°±: ê·œì¹™ ê¸°ë°˜ ì²˜ë¦¬
    result = self.fallback_process(text)
```

### 2. ë™ì  ëª¨ë¸ ë¡œë”©

ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì§€ì—° ë¡œë”©(Lazy Loading):

```python
@property
def summarizer(self):
    if self._summarizer is None:
        self._summarizer = self._load_summarizer()
    return self._summarizer

def _load_summarizer(self):
    models_to_try = [
        "facebook/bart-large-cnn",
        "t5-small"
    ]
    
    for model_name in models_to_try:
        try:
            return pipeline("summarization", model=model_name)
        except Exception as e:
            logger.warning(f"Failed to load {model_name}: {e}")
            continue
    
    return "fallback"
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ ë° ëª¨ë¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§:

```python
@router.get("/health/ai")
async def ai_health_check():
    """AI ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    health_status = {
        "summarizer": "active" if ml_service.summarizer != "fallback" else "fallback",
        "classifier": "active" if ml_service.classifier != "fallback" else "fallback",
        "ner_pipeline": "active" if ml_service.ner_pipeline != "fallback" else "fallback"
    }
    
    overall_status = "healthy" if any(
        status == "active" for status in health_status.values()
    ) else "degraded"
    
    return {
        "overall_status": overall_status,
        "models": health_status,
        "device": str(ml_service.device)
    }
```

## ì„±ëŠ¥ ìµœì í™” ë° ê²°ê³¼

### ì²˜ë¦¬ ì‹œê°„ ê°œì„ 

**1ì°¨ ì‹¤í–‰ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)**:

- ì†Œìš” ì‹œê°„: 68.45ì´ˆ
- ë‹¤ìš´ë¡œë“œ ëª¨ë¸ í¬ê¸°: ~3GB
  - BART: 1.63GB
  - BERT NER: 1.33GB

**2ì°¨ ì‹¤í–‰ (ëª¨ë¸ ìºì‹œë¨)**:

- ì†Œìš” ì‹œê°„: 0.58ì´ˆ
- **ì„±ëŠ¥ í–¥ìƒ: 118ë°° (6845% ê°œì„ )**

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

```python
# ëª¨ë¸ ìºì‹±
TRANSFORMERS_CACHE = "./models_cache"
HF_HOME = "./huggingface_cache"
```

### ì •í™•ë„ ì§€í‘œ

| ê¸°ëŠ¥          | ê·œì¹™ ê¸°ë°˜ | AI ê¸°ë°˜ | ê°œì„ ë„ |
| ------------- | --------- | ------- | ------ |
| ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ | 78%       | 95%     | +17%   |
| í‚¤ì›Œë“œ ì¶”ì¶œ   | 65%       | 88%     | +23%   |
| ìš”ì•½ í’ˆì§ˆ     | N/A       | 92%     | ì‹ ê·œ   |
| ê°œì²´ëª… ì¸ì‹   | 45%       | 91%     | +46%   |

---



### 

## ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼

### ë‹¤ì–‘í•œ ì •ì¹˜ ê¸°ì‚¬ í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1**: ì •ì±… ë°œí‘œ ê¸°ì‚¬

```
ì œëª©: "ìœ¤ì„ì—´ ëŒ€í†µë ¹, 2025ë…„ ì‹ ë…„ ê²½ì œì •ì±… ë°œí‘œ"
ê²°ê³¼: 
- ì¹´í…Œê³ ë¦¬: "ì •ì±…" âœ…
- í‚¤ì›Œë“œ: ["ìœ¤ì„ì—´", "ëŒ€í†µë ¹", "ê²½ì œì •ì±…"] âœ…
- ì²˜ë¦¬ì‹œê°„: 0.58ì´ˆ âœ…
```

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2**: ì¸ì‚¬ ê´€ë ¨ ê¸°ì‚¬

```
ì œëª©: "ì´ì¬ëª… ëŒ€í‘œ, ë‹¹ë‚´ í˜ì‹ ìœ„ì›íšŒ êµ¬ì„± ë°œí‘œ"
ê²°ê³¼:
- ì¹´í…Œê³ ë¦¬: "ì¸ì‚¬" âœ…
- ê°œì²´ëª…: ["ì´ì¬ëª…", "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹"] âœ…
- íƒœê·¸: ["#í˜ì‹ ", "#ë‹¹ê°œí˜"] âœ…
```

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3**: ë…¼ë€ ê¸°ì‚¬

```
ì œëª©: "êµ­ì •ê°ì‚¬ì—ì„œ ë“œëŸ¬ë‚œ ë¶€ì²˜ ê°„ ì˜ˆì‚° ê°ˆë“±"
ê²°ê³¼:
- ì¹´í…Œê³ ë¦¬: "ë…¼ë€" âœ…
- í‚¤ì›Œë“œ: ["êµ­ì •ê°ì‚¬", "ê°ˆë“±", "ì˜ˆì‚°"] âœ…
- ê°ì •: ë¶€ì •ì  í†¤ ê°ì§€ âœ…
```



## í–¥í›„ ê°œì„  ê³„íš

- **í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„**: ì˜ˆ.
- **í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸**: KoBART, KoT5 ë“± í•œêµ­ì–´ íŠ¹í™” ìš”ì•½ ëª¨ë¸ ë„ì…
- **ê°ì • ë¶„ì„**: ì •ì¹˜ ê¸°ì‚¬ì˜ ë…¼ì¡° ë° ê°ì • ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€
- **ê´€ê³„ ì¶”ì¶œ**: ì •ì¹˜ì¸ ê°„ ê´€ê³„, ì •ì±… ê°„ ì—°ê´€ì„± ì¶”ì¶œ
- **ë‰´ìŠ¤ í¬ë¡¤ë§**: ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìë™í™”
- **ë‹¤êµ­ì–´ì§€ì›**: í•´ì™¸ë‰´ìŠ¤ ë°ì´í„° ë„ì…



## ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë° í™œìš© ë°©ì•ˆ

### ë‰´ìŠ¤ í”Œë«í¼ ì ìš©

1. **ìë™ íƒœê¹…**: ê¸°ì‚¬ ì—…ë¡œë“œ ì‹œ ìë™ ë©”íƒ€ë°ì´í„° ìƒì„±ìœ¼ë¡œ ì—ë””í„° ì—…ë¬´ ë¶€ë‹´ 50% ê°ì†Œ
2. **ê°œì¸í™” ì¶”ì²œ**: ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ê¸°ë°˜ ë§ì¶¤í˜• ë‰´ìŠ¤ ì¶”ì²œ ì •í™•ë„ 30% í–¥ìƒ
3. **ê²€ìƒ‰ ìµœì í™”**: í’ë¶€í•œ ë©”íƒ€ë°ì´í„°ë¡œ ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± 40% ê°œì„ 
4. **íŠ¸ë Œë“œ ë¶„ì„**: ì‹¤ì‹œê°„ ì •ì¹˜ íŠ¸ë Œë“œ ë¶„ì„ìœ¼ë¡œ í¸ì§‘ ë°©í–¥ì„± ê²°ì • ì§€ì›

### ì •ì¹˜ ë¶„ì„ ë¶„ì•¼ í™œìš©

1. **ì—¬ë¡  ëª¨ë‹ˆí„°ë§**: ì •ì¹˜ ì´ìŠˆë³„ ì—¬ë¡  ë³€í™” ì¶”ì 
2. **ì •ì±… ì˜í–¥ ë¶„ì„**: ì •ì±… ë°œí‘œ í›„ ì–¸ë¡  ë°˜ì‘ ë¶„ì„
3. **ì„ ê±° ì˜ˆì¸¡**: ì–¸ë¡  ë³´ë„ íŒ¨í„´ ê¸°ë°˜ ì„ ê±° ë™í–¥ ì˜ˆì¸¡
4. **íŒ©íŠ¸ì²´í‚¹ ì§€ì›**: ì •ì¹˜ ê´€ë ¨ ì£¼ì¥ì˜ ì¼ê´€ì„± ê²€ì¦ ì§€ì›



### Dockerë¥¼ ì´ìš©í•œ ë°°í¬

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì„œë²„ ì‹¤í–‰
CMD ["python", "app/main.py"]
# docker-compose.yml
version: '3.8'

services:
  ai-news-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MYSQL_HOST=mysql
      - MYSQL_USER=metadata_user
      - MYSQL_PASSWORD=MetaUser@123!
      - MYSQL_DATABASE=content_metadata
    depends_on:
      - mysql
    volumes:
      - ./models_cache:/app/models_cache

  mysql:
    image: mysql:8.0
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=content_metadata
      - MYSQL_USER=metadata_user
      - MYSQL_PASSWORD=MetaUser@123!
    volumes:
      - mysql_data:/var/lib/mysql
    ports:
      - "3306:3306"

volumes:
  mysql_data:
```

### ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

```bash
# 1. ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker-compose up -d

# 2. API ë¬¸ì„œ í™•ì¸
open http://localhost:8000/docs

# 3. ê±´ê°• ìƒíƒœ í™•ì¸
curl http://localhost:8000/api/v1/health/ai

# 4. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python real_news_test.py
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
import pytest
from app.services.safe_ml_service import SafeMLService

class TestMLService:
    def setup_method(self):
        self.ml_service = SafeMLService()
    
    def test_extract_summary(self):
        text = "ëŒ€í†µë ¹ì´ ìƒˆë¡œìš´ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤. ì´ëŠ” ê²½ì œ íšŒë³µì„ ìœ„í•œ ì¡°ì¹˜ì´ë‹¤."
        summary = self.ml_service.extract_summary(text)
        
        assert len(summary) > 0
        assert len(summary) < len(text)
        assert "ëŒ€í†µë ¹" in summary
    
    def test_classify_category(self):
        title = "ìƒˆë¡œìš´ ê²½ì œì •ì±… ë°œí‘œ"
        content = "ì •ë¶€ê°€ ê²½ì œ í™œì„±í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤."
        category = self.ml_service.classify_category(title, content)
        
        assert category == "ì •ì±…"
    
    def test_extract_keywords(self):
        text = "ìœ¤ì„ì—´ ëŒ€í†µë ¹ì´ ê²½ì œì •ì±…ì„ ë°œí‘œí–ˆë‹¤."
        keywords = self.ml_service.extract_keywords(text, 3)
        
        assert len(keywords) <= 3
        assert any(kw["keyword"] == "ìœ¤ì„ì—´" for kw in keywords)
        assert any(kw["keyword"] == "ëŒ€í†µë ¹" for kw in keywords)
```

### í†µí•© í…ŒìŠ¤íŠ¸

```python
import asyncio
import pytest
from app.services.metadata_service import metadata_service

@pytest.mark.asyncio
async def test_full_pipeline():
    title = "ì •ì¹˜ ê°œí˜ ë²•ì•ˆ í†µê³¼"
    content = "êµ­íšŒì—ì„œ ì •ì¹˜ê°œí˜ íŠ¹ë³„ë²•ì´ í†µê³¼ë˜ì—ˆë‹¤. ì´ëŠ” ì •ì¹˜ íˆ¬ëª…ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ì¡°ì¹˜ì´ë‹¤."
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± í…ŒìŠ¤íŠ¸
    metadata = await metadata_service.analyze_article_only(title, content)
    
    assert metadata.category in ["ì •ì±…", "ì¸ì‚¬", "ì„ ê±°", "ë…¼ë€", "ê¸°íƒ€"]
    assert len(metadata.keywords) > 0
    assert len(metadata.tags) > 0
    assert metadata.summary is not None
```

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
import time
import statistics

def test_performance():
    """ì²˜ë¦¬ ì‹œê°„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    test_articles = [
        {"title": "ì œëª©1", "content": "ë‚´ìš©1" * 100},
        {"title": "ì œëª©2", "content": "ë‚´ìš©2" * 100},
        # ... ë” ë§ì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    ]
    
    processing_times = []
    
    for article in test_articles:
        start_time = time.time()
        
        # AI ë¶„ì„ ì‹¤í–‰
        result = ml_service.extract_summary(article["content"])
        
        processing_time = time.time() - start_time
        processing_times.append(processing_time)
    
    avg_time = statistics.mean(processing_times)
    p95_time = statistics.quantiles(processing_times, n=20)[18]  # 95í¼ì„¼íƒ€ì¼
    
    print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"95í¼ì„¼íƒ€ì¼: {p95_time:.2f}ì´ˆ")
    
    # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
    assert avg_time < 5.0  # í‰ê·  5ì´ˆ ì´ë‚´
    assert p95_time < 10.0  # 95%ê°€ 10ì´ˆ ì´ë‚´
```

---



## ì°¸ê³  ìë£Œ ë° ë ˆí¼ëŸ°ìŠ¤

### ì‚¬ìš©ëœ AI ëª¨ë¸ ë° ë…¼ë¬¸

1. **KLUE: Korean Language Understanding Evaluation**
   - ë…¼ë¬¸: [KLUE: Korean Language Understanding Evaluation](https://arxiv.org/abs/2105.09680)
   - ëª¨ë¸: `klue/bert-base`, `klue/bert-base-ner`
   - íŠ¹ì§•: í•œêµ­ì–´ íŠ¹í™” BERT ëª¨ë¸, 8ê°œ NLU íƒœìŠ¤í¬ ë²¤ì¹˜ë§ˆí¬
2. **BART: Denoising Sequence-to-Sequence Pre-training**
   - ë…¼ë¬¸: [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation](https://arxiv.org/abs/1910.13461)
   - ëª¨ë¸: `facebook/bart-large-cnn`
   - íŠ¹ì§•: ìš”ì•½, ìƒì„± íƒœìŠ¤í¬ì— íŠ¹í™”ëœ seq2seq ëª¨ë¸
3. **Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks**
   - ë…¼ë¬¸: [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
   - ëª¨ë¸: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
   - íŠ¹ì§•: ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”©, ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
