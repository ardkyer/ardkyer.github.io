---
layout: post
title: "insider trading agent 1"
date: 2025-12-19
typora-root-url: ../
image_style: "max-width:80%; display:block; margin:1em auto; border-radius:10px; box-shadow:2px 2px 8px rgba(0,0,0,0.8);"
---



# Insider Trading Alert Agent ê°œë°œê¸° - Day 1: ì¸í”„ë¼ êµ¬ì¶•ê³¼ SEC í¬ë¡¤ëŸ¬ êµ¬í˜„

## ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ**: SEC Form 4(ë‚´ë¶€ì ê±°ë˜ ê³µì‹œ)ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³ , Reddit/Twitter ê°ì„±ë¶„ì„ê³¼ ê²°í•©í•˜ì—¬ Slackìœ¼ë¡œ ì•Œë¦¼ì„ ë³´ë‚´ëŠ” AI Agent ì‹œìŠ¤í…œ êµ¬ì¶•

**ê¸°ìˆ  ìŠ¤íƒ**:

- **Orchestration**: Apache Airflow
- **Message Queue**: Apache Kafka
- **Database**: MySQL 8.0
- **Cache/Ranking**: Redis
- **Containerization**: Docker, Docker Compose
- **ì–¸ì–´**: Python 3.11

------

## ğŸ¯ ì˜¤ëŠ˜ì˜ ëª©í‘œ

1. âœ… í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„ ë° GitHub Repository ì„¸íŒ…
2. âœ… Docker ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶• (MySQL, Redis, Kafka, Zookeeper)
3. âœ… MySQL ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„
4. âœ… Apache Airflow í™˜ê²½ êµ¬ì¶•
5. âœ… SEC Form 4 í¬ë¡¤ëŸ¬ DAG êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸

------

## 1. í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„

### 1-1. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
insider-trading-agent/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/          # CI/CD (í–¥í›„ ì¶”ê°€)
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ sec_form4_crawler.py
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer/           # Kafka Producer (í–¥í›„ êµ¬í˜„)
â”‚   â””â”€â”€ consumer/           # Kafka Consumer (í–¥í›„ êµ¬í˜„)
â”œâ”€â”€ api/                    # FastAPI Dashboard (í–¥í›„ êµ¬í˜„)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_db.sql        # MySQL ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

### 1-2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEC EDGAR  â”‚ â† RSS Feed
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow   â”‚ â† DAG Scheduler (30ë¶„ë§ˆë‹¤)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Kafka    â”‚ â† Event Streaming
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚
â†“                 â†“
[Consumer 1]   [Consumer 2]
MySQL ì €ì¥     ê°ì„±ë¶„ì„
â”‚                 â”‚
â†“                 â†“
Redis ë­í‚¹      Slack ì•Œë¦¼
```

------

## 2. GitHub Repository ì´ˆê¸° ì„¸íŒ…

### 2-1. Git ì´ˆê¸°í™” ë° ê¸°ë³¸ íŒŒì¼ ìƒì„±

```bash
mkdir insider-trading-agent
cd insider-trading-agent
git init

# README.md ìƒì„±
cat > README.md << 'EOF'
# Insider Trading Alert Agent ğŸš¨

Real-time monitoring system for SEC Form 4 insider trading filings 
with community sentiment analysis.

## Features
- ğŸ” Real-time SEC Form 4 monitoring
- ğŸ“Š Reddit/Twitter sentiment analysis
- âš¡ Kafka-based event streaming
- ğŸ”” Slack alerts for top insider trades
- ğŸ“ˆ FastAPI dashboard

## Tech Stack
- **Orchestration**: Apache Airflow
- **Streaming**: Apache Kafka
- **Cache/Ranking**: Redis
- **Database**: MySQL
- **API**: FastAPI
- **Container**: Docker, Docker Compose
EOF
```

### 2-2. .gitignore ì„¤ì •

```bash
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
.Python
venv/

# Airflow
airflow/logs/
airflow/airflow.db

# Environment
.env
.env.local

# Database
*.sql.gz

# OS
.DS_Store
EOF
```

### 2-3. í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿

```bash
cat > .env.example << 'EOF'
# MySQL
MYSQL_ROOT_PASSWORD=rootpassword
MYSQL_DATABASE=insider_trading
MYSQL_USER=admin
MYSQL_PASSWORD=password

# Redis
REDIS_PASSWORD=redispassword

# Airflow
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__CORE__SQL_ALCHEMY_CONN=mysql+pymysql://admin:password@mysql:3306/insider_trading

# Kafka
KAFKA_BROKER_ID=1
KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181

# Slack (í–¥í›„ ì„¤ì •)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
EOF

cp .env.example .env
```

------

## 3. MySQL ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„

### 3-1. í…Œì´ë¸” ì„¤ê³„ ì² í•™

ë‚´ë¶€ì ê±°ë˜ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤:

- **ì‹œê³„ì—´ ë°ì´í„°**: ê±°ë˜ ë‚ ì§œê°€ ì¤‘ìš”
- **ê³ ìœ ì„±**: accession_numberë¡œ ì¤‘ë³µ ë°©ì§€
- **ê´€ê³„í˜•**: ì—¬ëŸ¬ í…Œì´ë¸” ê°„ JOIN í•„ìš”

### 3-2. í•µì‹¬ í…Œì´ë¸” 5ê°œ

```sql
-- 1. insider_trades: ë‚´ë¶€ì ê±°ë˜ ë©”ì¸ í…Œì´ë¸”
CREATE TABLE insider_trades (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    
    -- SEC Filing ì •ë³´
    accession_number VARCHAR(50) UNIQUE NOT NULL,
    filing_date DATE NOT NULL,
    
    -- ê¸°ì—… ì •ë³´
    ticker VARCHAR(10) NOT NULL,
    company_name VARCHAR(255) NOT NULL,
    cik VARCHAR(20),
    
    -- ë‚´ë¶€ì ì •ë³´
    insider_name VARCHAR(255) NOT NULL,
    insider_relationship VARCHAR(100),  -- CEO, CFO, Director ë“±
    is_director BOOLEAN DEFAULT FALSE,
    is_officer BOOLEAN DEFAULT FALSE,
    
    -- ê±°ë˜ ì •ë³´
    transaction_date DATE NOT NULL,
    transaction_code VARCHAR(10),
    transaction_type ENUM('BUY','SELL','OPTION','GIFT','OTHER'),
    shares BIGINT NOT NULL,
    price_per_share DECIMAL(15,4),
    transaction_value DECIMAL(20,2),
    shares_owned_after BIGINT,
    
    -- ë©”íƒ€ë°ì´í„°
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- ì¸ë±ìŠ¤
    INDEX idx_ticker (ticker),
    INDEX idx_filing_date (filing_date),
    INDEX idx_transaction_type (transaction_type)
);

-- 2. reddit_mentions: ì»¤ë®¤ë‹ˆí‹° ê°ì„± ë°ì´í„°
CREATE TABLE reddit_mentions (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    ticker VARCHAR(10) NOT NULL,
    post_id VARCHAR(50) UNIQUE NOT NULL,
    subreddit VARCHAR(100),
    title TEXT,
    sentiment ENUM('POSITIVE','NEGATIVE','NEUTRAL'),
    sentiment_score DECIMAL(5,4),
    posted_at TIMESTAMP NOT NULL,
    INDEX idx_ticker (ticker)
);

-- 3. daily_rankings: ì¼ë³„ ìƒìœ„ ê±°ë˜ (Redis ë°±ì—…ìš©)
CREATE TABLE daily_rankings (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    ranking_date DATE NOT NULL,
    rank_position INT NOT NULL,
    ticker VARCHAR(10) NOT NULL,
    insider_name VARCHAR(255),
    total_buy_value DECIMAL(20,2),
    UNIQUE KEY unique_daily_rank (ranking_date, rank_position)
);

-- 4. slack_alerts: ì•Œë¦¼ ë¡œê·¸
CREATE TABLE slack_alerts (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    ticker VARCHAR(10) NOT NULL,
    alert_type ENUM('LARGE_BUY','MULTIPLE_INSIDERS','HIGH_SENTIMENT'),
    alert_message TEXT,
    sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 5. processing_status: ë©±ë“±ì„± ë³´ì¥
CREATE TABLE processing_status (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    accession_number VARCHAR(50) UNIQUE NOT NULL,
    status ENUM('PENDING','PROCESSING','COMPLETED','FAILED'),
    processed_at TIMESTAMP NULL,
    INDEX idx_status (status)
);
```

### 3-3. ìƒ˜í”Œ ë°ì´í„° ì‚½ì…

```sql
INSERT INTO insider_trades (
    accession_number, filing_date, ticker, company_name, cik,
    insider_name, insider_relationship, is_officer,
    transaction_date, transaction_code, transaction_type,
    shares, price_per_share, transaction_value
) VALUES (
    '0001234567-25-000001',
    '2025-01-15',
    'NVDA',
    'NVIDIA Corporation',
    '0001234567',
    'Jensen Huang',
    'CEO',
    TRUE,
    '2025-01-14',
    'P',
    'BUY',
    10000,
    500.00,
    5000000.00
);
```

------

## 4. Docker Compose ì¸í”„ë¼ êµ¬ì¶•

### 4-1. í¬íŠ¸ ì¶©ëŒ í•´ê²° ê³¼ì •

**ë¬¸ì œ**: ë¡œì»¬ í™˜ê²½ì— MySQL(3306), Airflow Webserver(8080)ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘

**í•´ê²°**:

- MySQL: `3308:3306` (ë¡œì»¬ 3308 í¬íŠ¸ë¡œ ë§¤í•‘)
- Airflow: `8081:8080` (ë¡œì»¬ 8081 í¬íŠ¸ë¡œ ë§¤í•‘)

```bash
# í¬íŠ¸ ì‚¬ìš© í™•ì¸
lsof -i :3306
lsof -i :8080

# ëŒ€ì•ˆ í¬íŠ¸ë¡œ ë³€ê²½
```

### 4-2. docker-compose.yml ìµœì¢… ë²„ì „

```yaml
services:
  # MySQL 8.0
  mysql:
    image: mysql:8.0
    container_name: insider-mysql
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: insider_trading
      MYSQL_USER: admin
      MYSQL_PASSWORD: password
    ports:
      - "3308:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - insider-network
    command: --default-authentication-plugin=mysql_native_password

  # Redis
  redis:
    image: redis:7-alpine
    container_name: insider-redis
    command: redis-server --requirepass redispassword
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - insider-network

  # Zookeeper (Kafka ì˜ì¡´ì„±)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: insider-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - insider-network

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: insider-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - insider-network

  # Airflow Init (DB ë§ˆì´ê·¸ë ˆì´ì…˜ & ì‚¬ìš©ì ìƒì„±)
  airflow-init:
    build: ./airflow
    container_name: insider-airflow-init
    depends_on:
      - mysql
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: mysql+pymysql://admin:password@mysql:3306/insider_trading
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks:
      - insider-network
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com || true

  # Airflow Webserver
  airflow-webserver:
    build: ./airflow
    container_name: insider-airflow-webserver
    depends_on:
      - airflow-init
    ports:
      - "8081:8080"  # í¬íŠ¸ ì¶©ëŒ íšŒí”¼
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks:
      - insider-network
    command: webserver

  # Airflow Scheduler
  airflow-scheduler:
    build: ./airflow
    container_name: insider-airflow-scheduler
    depends_on:
      - airflow-init
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks:
      - insider-network
    command: scheduler

  # Airflow Worker (Celery Executor)
  airflow-worker:
    build: ./airflow
    container_name: insider-airflow-worker
    depends_on:
      - airflow-init
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    networks:
      - insider-network
    command: celery worker

networks:
  insider-network:
    driver: bridge

volumes:
  mysql_data:
  redis_data:
```

### 4-3. ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ë° í™•ì¸

```bash
# ì „ì²´ ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# ìƒíƒœ í™•ì¸
docker-compose ps

# ê²°ê³¼:
# insider-mysql          Up      3308->3306/tcp
# insider-redis          Up      6379/tcp
# insider-kafka          Up      9092/tcp
# insider-zookeeper      Up      2181/tcp
# insider-airflow-*      Up      8081->8080/tcp
```

### 4-4. MySQL ì ‘ì† í…ŒìŠ¤íŠ¸

```bash
# MySQL ì»¨í…Œì´ë„ˆ ì ‘ì†
docker exec -it insider-mysql mysql -u admin -ppassword -D insider_trading

# í…Œì´ë¸” í™•ì¸
mysql> SHOW TABLES;
+---------------------------+
| Tables_in_insider_trading |
+---------------------------+
| daily_rankings            |
| insider_trades            |
| processing_status         |
| reddit_mentions           |
| slack_alerts              |
+---------------------------+

# ìƒ˜í”Œ ë°ì´í„° í™•ì¸
mysql> SELECT ticker, insider_name, transaction_value 
       FROM insider_trades;
+--------+--------------+-------------------+
| ticker | insider_name | transaction_value |
+--------+--------------+-------------------+
| NVDA   | Jensen Huang |        5000000.00 |
+--------+--------------+-------------------+
```

------

## 5. Apache Airflow í™˜ê²½ êµ¬ì¶•

### 5-1. Airflow Dockerfile

```dockerfile
FROM apache/airflow:2.8.1-python3.11

USER root

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    gcc \
    python3-dev \
    && apt-get clean

USER airflow

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

# Airflow ì„¤ì •
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False
ENV AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
```

### 5-2. requirements.txt

```txt
# Airflow
apache-airflow==2.8.1
apache-airflow-providers-mysql==5.5.0
apache-airflow-providers-redis==3.5.0

# Kafka
kafka-python==2.0.2

# Database
pymysql==1.1.0
cryptography==41.0.7

# Web scraping
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0
feedparser==6.0.11

# Data processing
pandas==2.1.4
```

### 5-3. Airflow UI ì ‘ì†

```
URL: http://localhost:8081
ID: admin
PW: admin
```

------

## 6. SEC Form 4 í¬ë¡¤ëŸ¬ DAG êµ¬í˜„

### 6-1. SEC Form 4ë€?

**Form 4 (Statement of Changes in Beneficial Ownership)**

- íšŒì‚¬ ë‚´ë¶€ì(ì„ì›, ì´ì‚¬, 10% ì´ìƒ ëŒ€ì£¼ì£¼)ê°€ ìì‚¬ ì£¼ì‹ì„ ë§¤ë§¤í•  ë•Œ ì œì¶œ
- **ì œì¶œ ê¸°í•œ**: ê±°ë˜ í›„ 2ì˜ì—…ì¼ ì´ë‚´
- **ê³µê°œ ìœ„ì¹˜**: SEC EDGAR ì‹œìŠ¤í…œ
- **ì¤‘ìš”ì„±**: ë‚´ë¶€ìì˜ íšŒì‚¬ ì „ë§ì— ëŒ€í•œ ì‹ í˜¸ë¡œ í•´ì„ ê°€ëŠ¥

### 6-2. SEC EDGAR RSS êµ¬ì¡° ë¶„ì„

**RSS Feed URL**:

```
https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type=4&company=&dateb=&owner=include&start=0&count=100&output=atom
```

**ì£¼ìš” í•„ë“œ**:

- `entry.id`: Accession Number (ê³ ìœ  ì‹ë³„ì)
- `entry.link`: Form 4 HTML í˜ì´ì§€ URL
- `entry.title`: íšŒì‚¬ëª… ë° ë‚´ë¶€ì ì •ë³´
- `entry.summary`: ì œì¶œ ë‚ ì§œ, íŒŒì¼ í¬ê¸°

**ì£¼ì˜ì‚¬í•­**: SECëŠ” **User-Agent í—¤ë”**ë¥¼ í•„ìˆ˜ë¡œ ìš”êµ¬!

### 6-3. DAG êµ¬í˜„

```python
"""
SEC Form 4 Insider Trading Crawler DAG
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import feedparser
import requests
from bs4 import BeautifulSoup
import logging
import time

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'sec_form4_crawler',
    default_args=default_args,
    description='Crawl SEC Form 4 insider trading filings',
    schedule_interval='*/30 * * * *',  # 30ë¶„ë§ˆë‹¤ ì‹¤í–‰
    catchup=False,
    tags=['sec', 'insider-trading'],
)


def fetch_sec_rss(**context):
    """SEC EDGAR RSS í”¼ë“œì—ì„œ ìµœì‹  Form 4 ê°€ì ¸ì˜¤ê¸°"""
    
    logger = logging.getLogger(__name__)
    
    # SEC ìš”êµ¬ì‚¬í•­: User-Agent í•„ìˆ˜
    headers = {
        'User-Agent': 'InsiderTradingAgent/1.0 (your-email@example.com)',
        'Accept-Encoding': 'gzip, deflate',
        'Host': 'www.sec.gov'
    }
    
    rss_url = (
        "https://www.sec.gov/cgi-bin/browse-edgar"
        "?action=getcurrent&type=4&count=100&output=atom"
    )
    
    logger.info(f"Fetching RSS from: {rss_url}")
    
    # RSS ìš”ì²­
    response = requests.get(rss_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    logger.info(f"Response status: {response.status_code}")
    logger.info(f"Response length: {len(response.content)} bytes")
    
    # feedparserë¡œ íŒŒì‹±
    feed = feedparser.parse(response.content)
    
    logger.info(f"Feed entries: {len(feed.entries)}")
    
    filings = []
    for entry in feed.entries:
        filing = {
            'accession_number': entry.id.split('/')[-1],
            'filing_url': entry.link,
            'title': entry.title,
            'company': entry.title.split('-')[0].strip(),
            'filed_at': entry.get('published', ''),
            'summary': entry.get('summary', ''),
        }
        filings.append(filing)
    
    logger.info(f"âœ… Found {len(filings)} Form 4 filings")
    
    # XComì— ì €ì¥ (ë‹¤ìŒ Taskë¡œ ì „ë‹¬)
    context['task_instance'].xcom_push(key='filings', value=filings)
    
    return len(filings)


def parse_form4_details(**context):
    """Form 4 ìƒì„¸ ì •ë³´ íŒŒì‹±"""
    
    logger = logging.getLogger(__name__)
    
    # ì´ì „ Taskì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    filings = context['task_instance'].xcom_pull(
        task_ids='fetch_sec_rss',
        key='filings'
    )
    
    if not filings:
        logger.warning("No filings to process")
        return 0
    
    logger.info(f"Processing {len(filings)} filings")
    
    parsed_filings = []
    headers = {'User-Agent': 'InsiderTradingAgent/1.0'}
    
    # ì²˜ìŒ 5ê°œë§Œ íŒŒì‹± (í…ŒìŠ¤íŠ¸)
    for i, filing in enumerate(filings[:5]):
        try:
            logger.info(f"[{i+1}/5] Parsing: {filing['accession_number']}")
            
            # Rate limiting (SEC: 10 requests/sec)
            time.sleep(0.2)
            
            response = requests.get(
                filing['filing_url'],
                headers=headers,
                timeout=15
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            
            # ê°„ë‹¨í•œ ì •ë³´ë§Œ ì¶”ì¶œ (í–¥í›„ ê°œì„  ì˜ˆì •)
            parsed = {
                'accession_number': filing['accession_number'],
                'company': filing['company'],
                'html_length': len(response.text),
                'parsed_at': datetime.now().isoformat(),
            }
            
            parsed_filings.append(parsed)
            logger.info(f"âœ… Parsed: {filing['company']}")
            
        except Exception as e:
            logger.error(f"Error: {e}")
            continue
    
    logger.info(f"âœ… Successfully parsed {len(parsed_filings)} filings")
    
    context['task_instance'].xcom_push(
        key='parsed_filings',
        value=parsed_filings
    )
    
    return len(parsed_filings)


def send_to_kafka(**context):
    """íŒŒì‹±ëœ ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡ (í–¥í›„ êµ¬í˜„)"""
    
    logger = logging.getLogger(__name__)
    
    parsed_filings = context['task_instance'].xcom_pull(
        task_ids='parse_form4_details',
        key='parsed_filings'
    )
    
    if not parsed_filings:
        logger.warning("No parsed filings to send")
        return 0
    
    # TODO: ì‹¤ì œ Kafka Producer êµ¬í˜„
    for filing in parsed_filings:
        logger.info(f"ğŸ“¤ [KAFKA] Would send: {filing['company']}")
    
    return len(parsed_filings)


# Task ì •ì˜
task_fetch = PythonOperator(
    task_id='fetch_sec_rss',
    python_callable=fetch_sec_rss,
    dag=dag,
)

task_parse = PythonOperator(
    task_id='parse_form4_details',
    python_callable=parse_form4_details,
    dag=dag,
)

task_kafka = PythonOperator(
    task_id='send_to_kafka',
    python_callable=send_to_kafka,
    dag=dag,
)

# Task ì˜ì¡´ì„± ì„¤ì •
task_fetch >> task_parse >> task_kafka
```

### 6-4. DAG ì‹¤í–‰ ê²°ê³¼

**Airflow UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰ (Trigger DAG)**

#### Task 1: fetch_sec_rss

```
[2025-12-27, 03:22:42 UTC] INFO - Fetching RSS from: https://www.sec.gov/...
[2025-12-27, 03:22:43 UTC] INFO - Response status: 200
[2025-12-27, 03:22:43 UTC] INFO - Response length: 56083 bytes
[2025-12-27, 03:22:43 UTC] INFO - Feed entries: 100
[2025-12-27, 03:22:43 UTC] INFO - âœ… Found 100 Form 4 filings
```

#### Task 2: parse_form4_details

```
[2025-12-27, 03:22:43 UTC] INFO - Processing 100 filings
[2025-12-27, 03:22:43 UTC] INFO - [1/5] Parsing: urn:tag:sec.gov,2008:accession-number=0001193125-25-331321
[2025-12-27, 03:22:44 UTC] INFO - âœ… Parsed: IonQ, Inc.
...
[2025-12-27, 03:22:45 UTC] INFO - âœ… Successfully parsed 5 filings
```

#### Task 3: send_to_kafka

```
[2025-12-27, 03:22:45 UTC] INFO - Sending 5 filings to Kafka
[2025-12-27, 03:22:45 UTC] INFO - ğŸ“¤ [KAFKA] Would send: IonQ, Inc.
[2025-12-27, 03:22:45 UTC] INFO - âœ… Sent 5 filings to Kafka
```

**ê²°ê³¼**: ğŸ‰ **100ê°œ Form 4 ìˆ˜ì§‘, 5ê°œ íŒŒì‹± ì„±ê³µ!**

------

## 7. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê¸°ë¡

### ë¬¸ì œ 1: MySQL í¬íŠ¸ ì¶©ëŒ

**ì—ëŸ¬**: `Bind for 0.0.0.0:3306 failed: port is already allocated`

**ì›ì¸**: ë¡œì»¬ì— MySQLì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘

**í•´ê²°**:

```yaml
ports:
  - "3308:3306"  # ì™¸ë¶€ 3308 í¬íŠ¸ë¡œ ë³€ê²½
```

------

### ë¬¸ì œ 2: Airflow í¬íŠ¸ ì¶©ëŒ

**ì—ëŸ¬**: `Bind for 0.0.0.0:8080 failed: port is already allocated`

**í•´ê²°**:

```yaml
airflow-webserver:
  ports:
    - "8081:8080"  # ì™¸ë¶€ 8081 í¬íŠ¸ë¡œ ë³€ê²½
```

------

### ë¬¸ì œ 3: SEC RSSì—ì„œ ë°ì´í„° 0ê°œ

**ì—ëŸ¬**: `Found 0 Form 4 filings`

**ì›ì¸**: User-Agent í—¤ë” ëˆ„ë½

**í•´ê²°**:

```python
headers = {
    'User-Agent': 'InsiderTradingAgent/1.0 (your-email@example.com)',
    'Host': 'www.sec.gov'
}
response = requests.get(rss_url, headers=headers)
```

**ì¤‘ìš”**: SECëŠ” ë´‡ ë°©ì§€ë¥¼ ìœ„í•´ User-Agent ì²´í¬!

------

## 8. ì˜¤ëŠ˜ì˜ ì„±ê³¼

### âœ… ì™„ë£Œëœ ì‘ì—…

1. **ì¸í”„ë¼ êµ¬ì¶• ì™„ë£Œ**
   - Docker Composeë¡œ 7ê°œ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
   - MySQL, Redis, Kafka, Zookeeper, Airflow (webserver, scheduler, worker)
2. **ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì™„ë£Œ**
   - 5ê°œ í…Œì´ë¸” ìƒì„± (insider_trades, reddit_mentions, ë“±)
   - ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ
3. **Airflow DAG ì‘ë™ ê²€ì¦**
   - SEC RSSì—ì„œ 100ê°œ Form 4 ìˆ˜ì§‘ ì„±ê³µ
   - íŒŒì‹± íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
4. **Git ë²„ì „ ê´€ë¦¬**
   - GitHub Repository ì„¸íŒ…
   - .gitignore, README ì‘ì„±

### ğŸ“Š ì£¼ìš” ë©”íŠ¸ë¦­

- **í¬ë¡¤ë§ ì†ë„**: 100ê°œ Form 4 ìˆ˜ì§‘ ~1ì´ˆ
- **íŒŒì‹± ì‹œê°„**: 5ê°œ íŒŒì‹± ~2ì´ˆ (Rate Limit ì¤€ìˆ˜)
- **ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬**: ì´ ~2GB ì‚¬ìš©
- **DAG ì‹¤í–‰ ì£¼ê¸°**: 30ë¶„ë§ˆë‹¤ ìë™ ì‹¤í–‰

------

## 9. ë‹¤ìŒ ë‹¨ê³„ (Day 2 ê³„íš)

### ğŸ¯ ìš°ì„ ìˆœìœ„ ë†’ìŒ

1. **Form 4 XML íŒŒì„œ ì™„ì„±**
   - í‹°ì»¤(Ticker) ì¶”ì¶œ
   - ë‚´ë¶€ì ì´ë¦„, ì§ì±… íŒŒì‹±
   - ê±°ë˜ ìœ í˜•(ë§¤ìˆ˜/ë§¤ë„) êµ¬ë¶„
   - ê±°ë˜ ê¸ˆì•¡ ê³„ì‚°
2. **Kafka Producer êµ¬í˜„**
   - `raw-filings` Topic ìƒì„±
   - Form 4 ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡
   - ë©±ë“±ì„± ë³´ì¥ (ì¤‘ë³µ ë°©ì§€)
3. **Kafka Consumer êµ¬í˜„**
   - MySQL ì €ì¥ Consumer
   - Redis ë­í‚¹ Consumer

### ğŸ”® í–¥í›„ ê³„íš

1. **Reddit í¬ë¡¤ëŸ¬ ì¶”ê°€** (PRAW API)
2. **ê°ì„±ë¶„ì„ (FinBERT)**
3. **Slack ì•Œë¦¼ êµ¬í˜„**
4. **FastAPI ëŒ€ì‹œë³´ë“œ**
5. **ëª¨ë‹ˆí„°ë§ (Prometheus + Grafana)**

------

## 10. ë°°ìš´ ì  & íšŒê³ 

### ğŸ’¡ ê¸°ìˆ ì  ì¸ì‚¬ì´íŠ¸

1. **User-Agentì˜ ì¤‘ìš”ì„±**
   - SECê°™ì€ ê³µê³µê¸°ê´€ë„ ë´‡ ë°©ì§€ë¥¼ ìœ„í•´ User-Agent ì²´í¬
   - í¬ë¡¤ë§ ì‹œ í•­ìƒ ì ì ˆí•œ User-Agent ì„¤ì • í•„ìˆ˜
2. **Airflow XComì˜ í™œìš©**
   - Task ê°„ ë°ì´í„° ì „ë‹¬ì— XCom ì‚¬ìš©
   - ì‘ì€ ë°ì´í„°(<1MB)ì—ë§Œ ì í•©, í° ë°ì´í„°ëŠ” ì™¸ë¶€ ì €ì¥ì†Œ ì‚¬ìš©
3. **Docker í¬íŠ¸ ê´€ë¦¬**
   - ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ í¬íŠ¸ ì¶©ëŒ ë¹ˆë²ˆ
   - `docker-compose.yml`ì—ì„œ ì™¸ë¶€ í¬íŠ¸ ìœ ì—°í•˜ê²Œ ë³€ê²½

### ğŸš€ ìƒì‚°ì„± í–¥ìƒ íŒ

- **Docker Compose Logs**: `docker-compose logs -f [service]`ë¡œ ì‹¤ì‹œê°„ ë””ë²„ê¹…
- **Airflow Task Test**: `airflow tasks test [dag_id] [task_id] [date]`ë¡œ ê°œë³„ Task í…ŒìŠ¤íŠ¸
- **MySQL ë¹ ë¥¸ ì ‘ì†**: `docker exec -it insider-mysql mysql -u admin -p`

------

## 11. ì°¸ê³  ìë£Œ

- [SEC EDGAR API Documentation](https://www.sec.gov/edgar/sec-api-documentation)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Kafka Python Client](https://kafka-python.readthedocs.io/)
- [feedparser Documentation](https://feedparser.readthedocs.io/)

------

## ğŸ“ í”„ë¡œì íŠ¸ ì €ì¥ì†Œ

GitHub: [insider-trading-agent](https://github.com/YOUR_USERNAME/insider-trading-agent)

------

**ë‹¤ìŒ í¬ìŠ¤íŒ…**: Day 2 - Form 4 íŒŒì‹± & Kafka íŒŒì´í”„ë¼ì¸ êµ¬ì¶•



